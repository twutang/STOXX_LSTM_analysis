{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "thw116FYPmodel.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "qIU13nThWPq4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import datetime as dt\n",
        "\n",
        "import matplotlib.dates as mdates\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "from keras.models import Sequential, Model, load_model\n",
        "from keras.layers import Activation, Dense, LSTM, Dropout, Masking, Input\n",
        "from keras import optimizers\n",
        "from keras.utils.vis_utils import model_to_dot\n",
        "\n",
        "from IPython.display import SVG\n",
        "\n",
        "from google.colab import files\n",
        "\n",
        "import os"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7oerk0UaY0wG",
        "colab_type": "text"
      },
      "source": [
        "# Initial Data Importation and Structuring"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s-vX5NnVkai6",
        "colab_type": "text"
      },
      "source": [
        "## Accessing and loading data from Google Drive \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vtzJiK3GZxwf",
        "colab_type": "code",
        "outputId": "8428ce4d-4eae-4e6d-c8e6-4f780b355935",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 118
        }
      },
      "source": [
        "# mounting Google Drive which contains the relevant data\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tKuXkHkNWbSS",
        "colab_type": "code",
        "outputId": "a05e6eb9-083e-4a83-eeac-ceb04cc5397e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 505
        }
      },
      "source": [
        "# Fundamental input data\n",
        "ltm_book = pd.read_csv('/content/drive/My Drive/thw116_FYP/data/ltm_book.csv')\n",
        "ltm_div = pd.read_csv('/content/drive/My Drive/thw116_FYP/data/ltm_div.csv')\n",
        "ltm_ebit = pd.read_csv('/content/drive/My Drive/thw116_FYP/data/ltm_ebit.csv')\n",
        "ltm_eps = pd.read_csv('/content/drive/My Drive/thw116_FYP/data/ltm_eps.csv')\n",
        "ltm_fcf = pd.read_csv('/content/drive/My Drive/thw116_FYP/data/ltm_fcf.csv')\n",
        "ltm_pbook = pd.read_csv('/content/drive/My Drive/thw116_FYP/data/ltm_pbook.csv')\n",
        "ltm_sales = pd.read_csv('/content/drive/My Drive/thw116_FYP/data/ltm_sales.csv')\n",
        "\n",
        "ntm_book = pd.read_csv('/content/drive/My Drive/thw116_FYP/data/ntm_book.csv')\n",
        "ntm_div = pd.read_csv('/content/drive/My Drive/thw116_FYP/data/ntm_div.csv')\n",
        "ntm_ebit = pd.read_csv('/content/drive/My Drive/thw116_FYP/data/ntm_ebit.csv')\n",
        "ntm_eps = pd.read_csv('/content/drive/My Drive/thw116_FYP/data/ntm_eps.csv')\n",
        "ntm_fcf = pd.read_csv('/content/drive/My Drive/thw116_FYP/data/ntm_fcf.csv')\n",
        "ntm_pbook = pd.read_csv('/content/drive/My Drive/thw116_FYP/data/ntm_pbook.csv')\n",
        "ntm_sales = pd.read_csv('/content/drive/My Drive/thw116_FYP/data/ntm_sales.csv')\n",
        "\n",
        "# Technical input data\n",
        "\n",
        "price_high = pd.read_csv('/content/drive/My Drive/thw116_FYP/data/price_high.csv')\n",
        "price_low = pd.read_csv('/content/drive/My Drive/thw116_FYP/data/price_low.csv')\n",
        "price_open = pd.read_csv('/content/drive/My Drive/thw116_FYP/data/price_open.csv')\n",
        "volume = pd.read_csv('/content/drive/My Drive/thw116_FYP/data/volume.csv')\n",
        "enterprise_val = pd.read_csv('/content/drive/My Drive/thw116_FYP/data/enterprise_val.csv')\n",
        "market_cap = pd.read_csv('/content/drive/My Drive/thw116_FYP/data/market_cap.csv')\n",
        "\n",
        "print(ltm_book.head())\n",
        "print(\"**************************************\")\n",
        "print(ltm_book.info())\n",
        "\n",
        "# Price target output data\n",
        "\n",
        "price_close = pd.read_csv('/content/drive/My Drive/thw116_FYP/data/price_close.csv')\n",
        "\n",
        "print(price_close.head())\n",
        "print(\"**************************************\")\n",
        "print(price_close.info())"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "   Unnamed: 0  ATRS AV  ESSR LN  ...  FABGB SS  926002Q GY  NDA SS\n",
            "0           1      NaN      NaN  ...       NaN         NaN     NaN\n",
            "1           2      NaN      NaN  ...       NaN         NaN     NaN\n",
            "2           3      NaN      NaN  ...       NaN         NaN     NaN\n",
            "3           4      NaN      NaN  ...       NaN         NaN     NaN\n",
            "4           5      NaN      NaN  ...       NaN         NaN     NaN\n",
            "\n",
            "[5 rows x 1185 columns]\n",
            "**************************************\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 6313 entries, 0 to 6312\n",
            "Columns: 1185 entries, Unnamed: 0 to NDA SS\n",
            "dtypes: float64(1184), int64(1)\n",
            "memory usage: 57.1 MB\n",
            "None\n",
            "   Unnamed: 0  PAYS LN  CNHI IM     SGSN SW  ...  AMS SM  FER SM  SEV FP    FKI LN\n",
            "0  1995-01-03      NaN      NaN  222.552843  ...     NaN     NaN     NaN  1.896782\n",
            "1  1995-01-04      NaN      NaN  226.835424  ...     NaN     NaN     NaN  1.883390\n",
            "2  1995-01-05      NaN      NaN  228.312320  ...     NaN     NaN     NaN  1.910292\n",
            "3  1995-01-06      NaN      NaN  228.015450  ...     NaN     NaN     NaN  1.923861\n",
            "4  1995-01-09      NaN      NaN  228.680128  ...     NaN     NaN     NaN  1.925099\n",
            "\n",
            "[5 rows x 204 columns]\n",
            "**************************************\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 6313 entries, 0 to 6312\n",
            "Columns: 204 entries, Unnamed: 0 to FKI LN\n",
            "dtypes: float64(203), object(1)\n",
            "memory usage: 9.8+ MB\n",
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iM8LCiFbCWVG",
        "colab_type": "text"
      },
      "source": [
        "##Restructuring and Standardising the datasets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qwQDmx99oPzr",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 375
        },
        "outputId": "6e114442-dac3-4b98-b1da-2d7ac6998d0a"
      },
      "source": [
        "# Filtering incorrect datasets to suitable company subsets\n",
        "industrials_subset = price_close.columns.values.tolist()\n",
        "\n",
        "ltm_inputs = [ltm_book, ltm_div, ltm_ebit, ltm_eps, ltm_fcf, ltm_pbook, ltm_sales] \n",
        "ntm_inputs = [ntm_book, ntm_div, ntm_ebit, ntm_eps, ntm_fcf, ntm_pbook, ntm_sales]\n",
        "tech_inputs = [price_high, price_low, price_open, volume, enterprise_val, market_cap]\n",
        "\n",
        "for i in range(0,len(ltm_inputs)):\n",
        "  ltm_inputs[i] = ltm_inputs[i].loc[:, ltm_inputs[i].columns.str.contains('|'.join(industrials_subset))]\n",
        "  ltm_inputs[i]['Unnamed: 0'] = price_close['Unnamed: 0']\n",
        "  ltm_inputs[i].rename( columns={'Unnamed: 0':'Date'}, inplace=True )\n",
        "\n",
        "for i in range(0,len(ntm_inputs)):\n",
        "  ntm_inputs[i] = ntm_inputs[i].loc[:, ntm_inputs[i].columns.str.contains('|'.join(industrials_subset))]\n",
        "  ntm_inputs[i]['Unnamed: 0'] = price_close['Unnamed: 0']\n",
        "  ntm_inputs[i].rename( columns={'Unnamed: 0':'Date'}, inplace=True )\n",
        "\n",
        "for i in range(0,len(tech_inputs)):\n",
        "  tech_inputs[i] = tech_inputs[i].loc[:, tech_inputs[i].columns.str.contains('|'.join(industrials_subset))]\n",
        "  tech_inputs[i]['Unnamed: 0'] = price_close['Unnamed: 0']\n",
        "  tech_inputs[i].rename( columns={'Unnamed: 0':'Date'}, inplace=True )\n",
        "\n",
        "  \n",
        "price_close.rename( columns={'Unnamed: 0':'Date'}, inplace=True )"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:9: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
            "  if __name__ == '__main__':\n",
            "/usr/local/lib/python3.6/dist-packages/pandas/core/frame.py:4025: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
            "  return super(DataFrame, self).rename(**kwargs)\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:14: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
            "  \n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:19: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZhYPF4OmJOwm",
        "colab_type": "text"
      },
      "source": [
        "## Data Wrangling\n",
        "\n",
        "In order to produce a dataframe suitable for the model, we must first take the raw *Price.csv*  file and convert it to returns. This can then be maniuplated further in a variety of ways to achieve the neccessary classification for variants of the model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZHv1OtsgCC2C",
        "colab_type": "text"
      },
      "source": [
        "###Date Index Generator"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NcygJ5YtWP8H",
        "colab_type": "code",
        "outputId": "c4c1c2e4-f381-4c29-a29e-3fa5062086ba",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 115
        }
      },
      "source": [
        "# Conversion of TimeAndDate column to DatetimeIndex for ease of use\n",
        "\n",
        "def time_index_generator(dataframe):\n",
        "  dataframe['Date'] = pd.to_datetime(dataframe['Date'], dayfirst=True)\n",
        "  dataframe.set_index('Date', inplace=True)\n",
        "\n",
        "\n",
        "for i in range(0,len(ltm_inputs)):\n",
        "  time_index_generator(ltm_inputs[i])\n",
        "for i in range(0,len(ntm_inputs)):\n",
        "  time_index_generator(ntm_inputs[i])\n",
        "for i in range(0,len(tech_inputs)):\n",
        "  time_index_generator(tech_inputs[i])  \n",
        "\n",
        "time_index_generator(price_close)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:3: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
            "  This is separate from the ipykernel package so we can avoid doing imports until\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B5fWXKHjLpzc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Check all columns align correctly\n",
        "print(ltm_inputs[0].columns.difference(price_close.columns))\n",
        "print(ltm_inputs[1].columns.difference(price_close.columns))\n",
        "print(ltm_inputs[2].columns.difference(price_close.columns))\n",
        "print(ltm_inputs[3].columns.difference(price_close.columns))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VhIWXVtzB5co",
        "colab_type": "text"
      },
      "source": [
        "###Ticker Anonymisation \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hCGMHTY0zA7O",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Check that all tickers in the set match\n",
        "\n",
        "# Helper function to generate ticker list\n",
        "def ticker_list_generator(num):\n",
        "  ticker_list = []\n",
        "  for i in range(0, num):\n",
        "    ticker_name = 'Ticker ' + str(i+1)\n",
        "    ticker_list.append(ticker_name)\n",
        "  return ticker_list\n",
        "\n",
        "anon_tickers = ticker_list_generator(len(ltm_inputs[0].columns))\n",
        "\n",
        "for i in range(0,len(ltm_inputs)):\n",
        "  ltm_inputs[0].columns = anon_tickers\n",
        "for i in range(0,len(ntm_inputs)):\n",
        "  ntm_inputs[0].columns = anon_tickers\n",
        "for i in range(0,len(tech_inputs)):\n",
        "  tech_inputs[0].columns = anon_tickers\n",
        "\n",
        "price_close.columns = anon_tickers\n",
        "\n",
        "# # Selecting a Ticker and plotting\n",
        "\n",
        "# ltm_inputs[0]['Ticker 1'].plot(figsize=(16,6))\n",
        "# plt.title('ltm_book - Ticker 1')\n",
        "# plt.xlabel('Date')\n",
        "# plt.ylabel('Attribute')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r3bt2ejCCjYr",
        "colab_type": "text"
      },
      "source": [
        "###Price to Returns Converter"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gep9h1FNHQJ0",
        "colab_type": "code",
        "outputId": "41f6ac3d-6f93-4c8b-bee5-7b2aaa7c9939",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 115
        }
      },
      "source": [
        "# Converting Price format to returns format\n",
        "def price_to_returns(timeframe, dataframe):\n",
        "  if timeframe == 'daily':\n",
        "    return dataframe.pct_change(1) # remember to discount all target variables that are NaN\n",
        "  \n",
        "  if timeframe == 'monthly':\n",
        "    return dataframe.resample('BM')\n",
        "  \n",
        "returns = price_to_returns('daily', price_close) \n",
        "\n",
        "print(returns.head(2))"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "            Ticker 1  Ticker 2  Ticker 3  ...  Ticker 201  Ticker 202  Ticker 203\n",
            "Date                                      ...                                    \n",
            "1995-01-03       NaN       NaN       NaN  ...         NaN         NaN         NaN\n",
            "1995-01-04       NaN       NaN  0.019243  ...         NaN         NaN    -0.00706\n",
            "\n",
            "[2 rows x 203 columns]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "guJycK9f5Z1E",
        "colab_type": "text"
      },
      "source": [
        "# Data Preprocessing\n",
        "\n",
        "Several steps are taken in order to structure the data so that it can be proccessed by the model.\n",
        "\n",
        "1.   Data Analysis and Cleaning: Data is analysed for missing values, its properties, smoothed for various time intervals and resolved of any inconsistencies.  \n",
        "\n",
        "2. Training-Test Split: Data is divided into training and test splits according to a selected parameter value. \n",
        "\n",
        "3.  Data Transformation: Data is normalised, aggregated and generalised, both for training and testing\n",
        "\n",
        "4. Data Integration: Data is merged together appropriately to form the input shape of the model.   \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jfuTGvzQKPue",
        "colab_type": "text"
      },
      "source": [
        "##Data Analysis and Cleaning\n",
        "\n",
        "Something to consider is the fact that returns are more likely to correlate with changes in fundamentals rather than the absolute values of fundamentals. However, there is likely to be a disceprancy in the rate of change of fundamentals and the change in prices. Fundamentals are often only declared quarterly whereas prices are subject to daily fluctuations. We will first analyse the number of times a fundamental changes relative to the price changes. \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N-Drzbr-Blq1",
        "colab_type": "text"
      },
      "source": [
        "###Stastical Summaries of Data Count and Boxplots\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YqP4Ys_6c7vs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# statistical summary of datasets\n",
        "print(tech_inputs[0].describe())\n",
        "\n",
        "# Box plot of dataset's datapoints count\n",
        "tech_inputs[0].count().plot(kind='box', showmeans = True) \n",
        "plt.xlabel('Technical inputs')\n",
        "plt.tight_layout() \n",
        "plt.show()\n",
        "\n",
        "# plt.savefig('tech_box.png')\n",
        "# files.download('tech_box.png') \n",
        "\n",
        "# Individual tickers' datapoint count bar graph\n",
        "\n",
        "# ltm_inputs[0.count().plot(kind='barh', figsize=(20,20)) \n",
        "# plt.xlabel('LTM input')\n",
        "# plt.tight_layout() \n",
        "# plt.show()\n",
        "\n",
        "# plt.savefig('ltmTickerCount.png')\n",
        "# files.download('ltmTickerCount.png') "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YsXsCknQBv-l",
        "colab_type": "text"
      },
      "source": [
        "###Stastical Summaries of Delta Data Count and Boxplots\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XO_BJonjhHVL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def data_change(dataframe, column): \n",
        "  noChange_count = 0\n",
        "  NaN_count = 0\n",
        "  for i in range (0, len(dataframe.index)): \n",
        "    if dataframe[column].diff().iloc[i] == 0:\n",
        "      noChange_count += 1\n",
        "    if dataframe[column].isna().iloc[i]: \n",
        "      NaN_count += 1\n",
        "  \n",
        "#   print('Total rows: ', len(dataframe[column]))\n",
        "#   print('The number of rows where no change occurs: ', noChange_count)\n",
        "#   print('The number of rows which are NaN: ', NaN_count)\n",
        "#   print('Useful datapoints:', (len(dataframe[column])-noChange_count-NaN_count))\n",
        "\n",
        "  return len(dataframe[column])-noChange_count-NaN_count\n",
        "\n",
        "delta_change = []\n",
        "\n",
        "for companies in range(0, tech_inputs[0].shape[1]):\n",
        "  delta_val = data_change(tech_inputs[0], anon_tickers[companies])\n",
        "  delta_change.append(delta_val)\n",
        "  \n",
        "plt.boxplot(delta_change, showmeans = True)\n",
        "plt.xlabel('Technical inputs')\n",
        "plt.show()\n",
        "\n",
        "# Plotting the distributions of the returns datapoints. \n",
        "\n",
        "allreturns = returns.stack(dropna=False).reset_index(drop=True).to_frame('Log Returns Plot')\n",
        "allreturns.hist(range=(-0.2,0.2), figsize=(16,8), bins=200)\n",
        "\n",
        "allreturns = np.log(allreturns) \n",
        "allreturns.hist(range=(-10,0), figsize=(16,8), bins=200)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BY93g3OxY7uf",
        "colab_type": "text"
      },
      "source": [
        "## Data Snipping"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MtPwvxIHJYUJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# # Entire dataset\n",
        "# for i in range(0,len(ltm_inputs)):\n",
        "#   ltm_inputs[i] = ltm_inputs[i].loc[ltm_inputs[i].index > pd.to_datetime('2000-1-1')]\n",
        "\n",
        "# for i in range(0,len(ntm_inputs)):\n",
        "#   ntm_inputs[i] = ntm_inputs[i].loc[ntm_inputs[i].index > pd.to_datetime('2000-1-1')]\n",
        "\n",
        "# for i in range(0,len(tech_inputs)):\n",
        "#   tech_inputs[i] = tech_inputs[i].loc[tech_inputs[i].index > pd.to_datetime('2000-1-1')]\n",
        "  \n",
        "# returns = returns.loc[returns.index > pd.to_datetime('2000-1-1')]\n",
        "\n",
        "\n",
        "# Reduced subset for faster training and testing\n",
        "\n",
        "for i in range(0,len(ltm_inputs)):\n",
        "  ltm_inputs[i] = ltm_inputs[i].loc[ltm_inputs[i].index > pd.to_datetime('2015-1-1')]\n",
        "\n",
        "for i in range(0,len(ntm_inputs)):\n",
        "  ntm_inputs[i] = ntm_inputs[i].loc[ntm_inputs[i].index > pd.to_datetime('2015-1-1')]\n",
        "\n",
        "for i in range(0,len(tech_inputs)):\n",
        "  tech_inputs[i] = tech_inputs[i].loc[tech_inputs[i].index > pd.to_datetime('2015-1-1')]\n",
        "  \n",
        "returns = returns.loc[returns.index > pd.to_datetime('2010-1-1')]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XbrJzj6fJqvv",
        "colab_type": "text"
      },
      "source": [
        "## Data splitting"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2BEO_I7KRqr_",
        "colab_type": "text"
      },
      "source": [
        "### Training set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4yfDmdQJcrPp",
        "colab_type": "code",
        "outputId": "1ff81314-2251-47ac-87f2-42f1f06cda9c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 228
        }
      },
      "source": [
        "def training_set(train_percentage, dataframe): \n",
        "  train_size = int(train_percentage*len(dataframe.index)) \n",
        "  train_set = dataframe[:train_size]\n",
        "  return pd.DataFrame(train_set)  \n",
        "\n",
        "training_split = 0.6\n",
        "\n",
        "# INPUTS\n",
        "\n",
        "print(type(ltm_inputs))\n",
        "\n",
        "ltm_trainInputs = []\n",
        "ntm_trainInputs = []\n",
        "tech_trainInputs = []\n",
        "\n",
        "for i in range(0,len(ltm_inputs)):\n",
        "  ltm_trainInputs.append(training_set(training_split, ltm_inputs[i]))\n",
        "  \n",
        "for i in range(0,len(ntm_inputs)):\n",
        "  ntm_trainInputs.append(training_set(training_split, ntm_inputs[i]))\n",
        "\n",
        "for i in range(0,len(tech_inputs)):\n",
        "  tech_trainInputs.append(training_set(training_split, tech_inputs[i]))\n",
        "\n",
        "  \n",
        "returns_trainOutput = training_set(training_split, returns)\n",
        "\n",
        "print(ntm_trainInputs[0].info())\n",
        "print(returns_trainOutput.info())"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'list'>\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "DatetimeIndex: 1454 entries, 2010-01-04 to 2015-08-13\n",
            "Columns: 203 entries, Ticker 1 to Ticker 203\n",
            "dtypes: float64(203)\n",
            "memory usage: 2.3 MB\n",
            "None\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "DatetimeIndex: 1454 entries, 2010-01-04 to 2015-08-13\n",
            "Columns: 203 entries, Ticker 1 to Ticker 203\n",
            "dtypes: float64(203)\n",
            "memory usage: 2.3 MB\n",
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "47IfciNTutCX",
        "colab_type": "text"
      },
      "source": [
        "### Validation Set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RJ-QTlDF1hOm",
        "colab_type": "code",
        "outputId": "d2059f34-d4c0-4bfc-a8ef-efa7f9759756",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 212
        }
      },
      "source": [
        "def validation_set(train_percentage, val_percentage, dataframe): \n",
        "  train_size = int(train_percentage*len(dataframe.index)) \n",
        "  val_size = int(val_percentage*len(dataframe.index))\n",
        "  # val_size = len(dataframe.index)-int(train_percentage*len(dataframe.index)) \n",
        "  val_set = dataframe[train_size:(train_size+val_size)]\n",
        "  return pd.DataFrame(val_set)  \n",
        "\n",
        "validation_split = 0.2\n",
        "\n",
        "ltm_valInputs = []\n",
        "ntm_valInputs = []\n",
        "tech_valInputs = []\n",
        "\n",
        "for i in range(0,len(ltm_inputs)):\n",
        "  ltm_valInputs.append(validation_set(training_split, validation_split, ltm_inputs[i]))\n",
        "  \n",
        "for i in range(0,len(ntm_inputs)):\n",
        "  ntm_valInputs.append(validation_set(training_split, validation_split, ntm_inputs[i]))\n",
        "\n",
        "for i in range(0,len(tech_inputs)):\n",
        "  tech_valInputs.append(validation_set(training_split, validation_split, tech_inputs[i]))\n",
        "\n",
        "  \n",
        "returns_valOutput = validation_set(training_split, validation_split, returns)\n",
        "\n",
        "print(ltm_valInputs[0].info())\n",
        "print(returns_valOutput.info())"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "DatetimeIndex: 484 entries, 2015-08-14 to 2017-06-28\n",
            "Columns: 203 entries, Ticker 1 to Ticker 203\n",
            "dtypes: float64(203)\n",
            "memory usage: 771.4 KB\n",
            "None\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "DatetimeIndex: 484 entries, 2015-08-14 to 2017-06-28\n",
            "Columns: 203 entries, Ticker 1 to Ticker 203\n",
            "dtypes: float64(203)\n",
            "memory usage: 771.4 KB\n",
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bYvSLMz9RiA9",
        "colab_type": "text"
      },
      "source": [
        "### Test Set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ypx6Ia7uRhjS",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 212
        },
        "outputId": "1b737db4-435e-48e7-ca9e-00c32b93c279"
      },
      "source": [
        "def test_set(train_percentage, val_percentage, dataframe): \n",
        "  test_percentage = 1-train_percentage-val_percentage\n",
        "  train_size = int(train_percentage*len(dataframe.index)) \n",
        "  val_size = int(val_percentage*len(dataframe.index))\n",
        "  # val_size = len(dataframe.index)-int(train_percentage*len(dataframe.index)) \n",
        "  test_set = dataframe[(train_size+val_size):]\n",
        "  return pd.DataFrame(test_set) \n",
        "\n",
        "ltm_testInputs = []\n",
        "ntm_testInputs = []\n",
        "tech_testInputs = []\n",
        "\n",
        "for i in range(0,len(ltm_inputs)):\n",
        "  ltm_testInputs.append(test_set(training_split, validation_split, ltm_inputs[i]))\n",
        "  \n",
        "for i in range(0,len(ntm_inputs)):\n",
        "  ntm_testInputs.append(test_set(training_split, validation_split, ntm_inputs[i]))\n",
        "\n",
        "for i in range(0,len(tech_inputs)):\n",
        "  tech_testInputs.append(test_set(training_split, validation_split, tech_inputs[i]))\n",
        "\n",
        "returns_testOutput = test_set(training_split, validation_split, returns)\n",
        "\n",
        "print(ltm_testInputs[0].info())\n",
        "print(returns_testOutput.info())  "
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "DatetimeIndex: 486 entries, 2017-06-29 to 2019-05-17\n",
            "Columns: 203 entries, Ticker 1 to Ticker 203\n",
            "dtypes: float64(203)\n",
            "memory usage: 774.6 KB\n",
            "None\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "DatetimeIndex: 486 entries, 2017-06-29 to 2019-05-17\n",
            "Columns: 203 entries, Ticker 1 to Ticker 203\n",
            "dtypes: float64(203)\n",
            "memory usage: 774.6 KB\n",
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NMx_2j-80Bfi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "fig = plt.figure()\n",
        "fig.set_figheight(28)\n",
        "\n",
        "plt.subplot(4, 1, 1)\n",
        "sns.heatmap(returns_trainOutput.isnull(), cbar=False)\n",
        "\n",
        "plt.subplot(4, 1, 2)\n",
        "sns.heatmap(returns_valOutput.isnull(), cbar=False)\n",
        "\n",
        "plt.subplot(4, 1, 3)\n",
        "sns.heatmap(tech_trainInputs[1].isnull(), cbar=False)\n",
        "\n",
        "plt.subplot(4, 1, 4)\n",
        "sns.heatmap(tech_valInputs[1].isnull(), cbar=False)\n",
        "\n",
        "# plt.savefig('heatmap.png')\n",
        "# files.download('heatmap.png') "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U79APBT7JwcE",
        "colab_type": "text"
      },
      "source": [
        "## Input and Output Transformation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LfE-0om8FrZz",
        "colab_type": "text"
      },
      "source": [
        "###Scaling"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "38KlbkcIfOk4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def input_scaling(dataframe):\n",
        "  \n",
        "  # This is the MinMax Scaling function \n",
        "  sc = MinMaxScaler(feature_range = (0, 1))\n",
        "  scaled_input_dataframe = sc.fit_transform(dataframe) # This is now an n-dimensional array type\n",
        "  \n",
        "  transformer = Normalizer().fit(dataframe)\n",
        "  Normalizer(copy=True, norm='l2')\n",
        "  transformer.transform(X)\n",
        "  \n",
        "  return scaled_input_dataframe\n",
        "\n",
        "np.warnings.filterwarnings('ignore', r'All-NaN (slice|axis) encountered')\n",
        "\n",
        "for i in range(0,len(ltm_inputs)):\n",
        "  ltm_trainInputs[i] = input_scaling(ltm_trainInputs[i])\n",
        "  ltm_valInputs[i] = input_scaling(ltm_valInputs[i])\n",
        "  ltm_testInputs[i] = input_scaling(ltm_testInputs[i])\n",
        "  \n",
        "for i in range(0,len(ntm_inputs)):\n",
        "  ntm_trainInputs[i] = input_scaling(ntm_trainInputs[i])\n",
        "  ntm_valInputs[i] = input_scaling(ntm_valInputs[i])\n",
        "  ntm_testInputs[i] = input_scaling(ntm_testInputs[i])\n",
        "\n",
        "for i in range(0,len(tech_inputs)):\n",
        "  tech_trainInputs[i] = input_scaling(tech_trainInputs[i])\n",
        "  tech_valInputs[i] = input_scaling(tech_valInputs[i])\n",
        "  tech_testInputs[i] = input_scaling(tech_testInputs[i])\n",
        "  \n",
        "# sample_input = [ltm_trainInputs[0][1000:1001][0][3],ntm_trainInputs[0][1000:1001][0][3], tech_trainInputs[0][1000:1001][0][3] ]\n",
        "# print(sample_input)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F6snYzXc-yTv",
        "colab_type": "code",
        "outputId": "5413951c-771d-4dd8-ab10-c1d3074f212b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 357
        }
      },
      "source": [
        "def output_classifier(type ,dataframe): \n",
        "  \n",
        "  # No adjustments\n",
        "  if type == 'raw':\n",
        "    scaled_dataframe = dataframe\n",
        "  \n",
        "  # Binary classifier where return>0 is +1, and return<0 is 0 labels\n",
        "  if type == 'binary':\n",
        "    pos_returns = dataframe.values > 0\n",
        "    neg_returns = dataframe.values <= 0 \n",
        "    scaled_dataframe = pd.DataFrame(np.select([pos_returns,neg_returns], [1,0], default='NaN'), index=dataframe.index, columns=dataframe.columns)\n",
        "\n",
        "  return scaled_dataframe\n",
        "\n",
        "returns_trainOutput = output_classifier('binary', returns_trainOutput)\n",
        "returns_valOutput = output_classifier('binary', returns_valOutput)\n",
        "returns_testOutput = output_classifier('binary', returns_testOutput)\n",
        "\n",
        "print (returns_trainOutput.head(2))\n",
        "print (returns_trainOutput.info())\n",
        "\n",
        "print (returns_valOutput.head(2))\n",
        "print (returns_valOutput.info())"
      ],
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-52-cec2fa57714a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mscaled_dataframe\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m \u001b[0mreturns_trainOutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutput_classifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'binary'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturns_trainOutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0mreturns_valOutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutput_classifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'binary'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturns_valOutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0mreturns_testOutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutput_classifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'binary'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturns_testOutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-52-cec2fa57714a>\u001b[0m in \u001b[0;36moutput_classifier\u001b[0;34m(type, dataframe)\u001b[0m\n\u001b[1;32m      7\u001b[0m   \u001b[0;31m# Binary classifier where return>0 is +1, and return<0 is 0 labels\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mtype\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'binary'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0mpos_returns\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdataframe\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m     \u001b[0mneg_returns\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdataframe\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0mscaled_dataframe\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpos_returns\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mneg_returns\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdefault\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'NaN'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdataframe\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdataframe\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: '>' not supported between instances of 'str' and 'int'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xeMN7TE0Ki1P",
        "colab_type": "text"
      },
      "source": [
        "## Data Integration"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cYKnA5iUAnmG",
        "colab_type": "code",
        "outputId": "b1dd961c-d011-4954-81e3-8cc49cfc0248",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 232
        }
      },
      "source": [
        "trainInput = []\n",
        "trainTarget = []\n",
        "\n",
        "valInput = []\n",
        "valTarget = []\n",
        "\n",
        "testInput = []\n",
        "testTarget = []\n",
        "\n",
        "# Check that the indices are of the same length \n",
        "if len(ltm_trainInputs[0]) != len(returns_trainOutput):\n",
        "  print ('training length', len(ltm_trainInputs[0]))\n",
        "  print ('target length', len(returns_trainOutput))\n",
        "  assert False, \"Incompatible dataframe index lengths!\"\n",
        "\n",
        "# Training Set\n",
        "for company in range(0, len(ltm_trainInputs[0][:1][0])):\n",
        "  \n",
        "  for time_unit in range(0, len(ltm_trainInputs[0])): \n",
        "    input_unit = []\n",
        "    \n",
        "    for ltm_attribute in range(0, len(ltm_trainInputs)): \n",
        "      input_unit.append(ltm_trainInputs[ltm_attribute][time_unit:time_unit+1][0][company])\n",
        "                        \n",
        "    for ntm_attribute in range(0, len(ntm_trainInputs)):\n",
        "      input_unit.append(ntm_trainInputs[ntm_attribute][time_unit:time_unit+1][0][company])\n",
        "                        \n",
        "    for tech_attribute in range(0, len(tech_trainInputs)): \n",
        "      input_unit.append(tech_trainInputs[tech_attribute][time_unit:time_unit+1][0][company])\n",
        "                        \n",
        "    trainInput.append(input_unit)\n",
        "    \n",
        "for company in range(0, len(returns_trainOutput.columns)):\n",
        "  for time_unit in range(0, len(returns_trainOutput.index)): \n",
        "    trainTarget.append(returns_trainOutput[anon_tickers[company]][time_unit])\n",
        "  \n",
        "# Validation Set \n",
        "for company in range(0, len(ltm_valInputs[0][:1][0])):\n",
        "  \n",
        "  for time_unit in range(0, len(ltm_valInputs[0])): \n",
        "    input_unit = []\n",
        "    \n",
        "    for ltm_attribute in range(0, len(ltm_valInputs)): \n",
        "      input_unit.append(ltm_valInputs[ltm_attribute][time_unit:time_unit+1][0][company])\n",
        "                        \n",
        "    for ntm_attribute in range(0, len(ntm_trainInputs)):\n",
        "      input_unit.append(ntm_valInputs[ntm_attribute][time_unit:time_unit+1][0][company])\n",
        "                        \n",
        "    for tech_attribute in range(0, len(tech_trainInputs)): \n",
        "      input_unit.append(tech_valInputs[tech_attribute][time_unit:time_unit+1][0][company])\n",
        "                        \n",
        "    valInput.append(input_unit)\n",
        "    \n",
        "for company in range(0, len(returns_valOutput.columns)):\n",
        "  for time_unit in range(0, len(returns_valOutput.index)): \n",
        "    valTarget.append(returns_valOutput[anon_tickers[company]][time_unit])\n",
        "\n",
        "# Test Set\n",
        "for company in range(0, len(ltm_testInputs[0][:1][0])):\n",
        "  \n",
        "  for time_unit in range(0, len(ltm_testInputs[0])): \n",
        "    input_unit = []\n",
        "    \n",
        "    for ltm_attribute in range(0, len(ltm_testInputs)): \n",
        "      input_unit.append(ltm_testInputs[ltm_attribute][time_unit:time_unit+1][0][company])\n",
        "                        \n",
        "    for ntm_attribute in range(0, len(ntm_testInputs)):\n",
        "      input_unit.append(ntm_testInputs[ntm_attribute][time_unit:time_unit+1][0][company])\n",
        "                        \n",
        "    for tech_attribute in range(0, len(tech_testInputs)): \n",
        "      input_unit.append(tech_testInputs[tech_attribute][time_unit:time_unit+1][0][company])\n",
        "                        \n",
        "    testInput.append(input_unit)\n",
        "    \n",
        "for company in range(0, len(returns_testOutput.columns)):\n",
        "  for time_unit in range(0, len(returns_testOutput.index)): \n",
        "    testTarget.append(returns_testOutput[anon_tickers[company]][time_unit])\n",
        "\n",
        "\n",
        "print(trainInput[0:1])\n",
        "print(len(trainInput))\n",
        "print(trainTarget[0:1])\n",
        "print(len(trainTarget))\n",
        "\n",
        "print(valInput[0:1])\n",
        "print(len(valInput))\n",
        "print(valTarget[0:1])\n",
        "print(len(valTarget))\n",
        "\n",
        "print(testInput[0:1])\n",
        "print(len(testInput))\n",
        "print(testTarget[0:1])\n",
        "print(len(testTarget))"
      ],
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[0.3287671232876711, 0.0, 0.0034439322249904397, 0.0, 0.27587890625, 0.024924571101988854, 0.0013178703215603604, 0.023927191685372606, 0.0, 0.16577420722121472, 0.15657919955906302, 0.07080194831886986, 0.06567425221342738, 0.06785172510001797, 0.01701016959638503, 0.022328109127080567, 0.02119637478718763, 7.721563054542674e-05, 0.0011800881756525248, 0.007578283773109033]]\n",
            "295162\n",
            "['0']\n",
            "295162\n",
            "[[0.0, 0.0, 0.006616800920598409, 1.0, 0.0, 1.0, 0.0, 0.0, 0.9782756086911765, 0.0, 0.0, 0.0, 1.0, 0.0, 0.212430052227369, 0.382723829490314, 0.2704902616832283, 0.0561259051719461, 0.23309289364037244, 0.22129841273113016]]\n",
            "98252\n",
            "['1']\n",
            "98252\n",
            "[[0.0, 0.0, 0.0, 0.0, 0.0, 0.03575449848301826, 0.0, 0.0, 0.0, 0.0, 0.0, 0.26105637662522807, 0.0964890882613485, 0.0, 0.1188318300971618, 0.05611671045839817, 0.18879781256027162, 0.14282616915785304, 0.006907985192873944, 0.006135475514882138]]\n",
            "98658\n",
            "[-0.026994255216504892]\n",
            "98658\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pLRv87o_Z9ME",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        },
        "outputId": "885781a9-318d-4587-f726-42ff83291a60"
      },
      "source": [
        "np.array_equal(trainInput,trainInputTest)"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "False"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wtn_1CnkXq_u",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 115
        },
        "outputId": "0385e02e-ee09-4809-99de-b7c1df9eb206"
      },
      "source": [
        "np.save(\"trainInput.npy\", trainInput)\n",
        "\n",
        "trainInputTest = np.load(\"trainInput.npy\")\n",
        "print(trainInputTest[0:1])\n",
        "print(len(trainInputTest))"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[3.28767123e-01 0.00000000e+00 3.44393222e-03 0.00000000e+00\n",
            "  2.75878906e-01 2.49245711e-02 1.31787032e-03 2.39271917e-02\n",
            "  0.00000000e+00 1.65774207e-01 1.56579200e-01 7.08019483e-02\n",
            "  6.56742522e-02 6.78517251e-02 1.70101696e-02 2.23281091e-02\n",
            "  2.11963748e-02 7.72156305e-05 1.18008818e-03 7.57828377e-03]]\n",
            "295162\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6uL1FAaxCxMm",
        "colab_type": "text"
      },
      "source": [
        "### Remove redundant data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R4J19cJ1_k_8",
        "colab_type": "code",
        "outputId": "854829e9-aae8-4a49-994c-98ea7a81838a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "source": [
        "#239637\n",
        "#269977\n",
        "#166449\n",
        "\n",
        "def remove_useless_inputs(inputList, outputList):\n",
        "  resultInput = []\n",
        "  resultOutput = []\n",
        "  for i in range (0, len(inputList)):\n",
        "    nancount = 0\n",
        "    for val in range(0, len(inputList[i])):\n",
        "      if str(inputList[i:i+1][0][val]) == 'nan': \n",
        "        nancount += 1\n",
        "#     if nancount < len(inputList[i:i+1][0]): \n",
        "    if nancount < 1:\n",
        "      resultInput.append(inputList[i])\n",
        "      resultOutput.append(outputList[i])\n",
        "      \n",
        "  return resultInput, resultOutput\n",
        "\n",
        "newtrainInput, newtrainTarget = remove_useless_inputs(trainInput, trainTarget)\n",
        "newvalInput, newvalTarget = remove_useless_inputs(valInput, valTarget)\n",
        "newtestInput, newtestTarget = remove_useless_inputs(testInput, testTarget)\n",
        "\n",
        "\n",
        "print(newtrainInput[0:1])\n",
        "print(len(newtrainInput))\n",
        "print(newtrainTarget[0:1])\n",
        "print(len(newtrainTarget))\n"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[0.3287671232876711, 0.0, 0.0034439322249904397, 0.0, 0.27587890625, 0.024924571101988854, 0.0013178703215603604, 0.023927191685372606, 0.0, 0.16577420722121472, 0.15657919955906302, 0.07080194831886986, 0.06567425221342738, 0.06785172510001797, 0.01701016959638503, 0.022328109127080567, 0.02119637478718763, 7.721563054542674e-05, 0.0011800881756525248, 0.007578283773109033]]\n",
            "166449\n",
            "['0']\n",
            "166449\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HfihFidpCu6B",
        "colab_type": "code",
        "outputId": "869e79c6-e5b1-45da-dfdb-c79aabe3f16d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 183
        }
      },
      "source": [
        "# 239635\n",
        "# 253107\n",
        "# 166448\n",
        "\n",
        "def remove_useless_outputs(inputList, outputList):\n",
        "  resultInput = []\n",
        "  resultOutput = []\n",
        "  for i in range (0, len(outputList)):\n",
        "    if str(outputList[i:i+1][0]).lower() != 'nan': \n",
        "      resultInput.append(inputList[i])\n",
        "      resultOutput.append(outputList[i])\n",
        "   \n",
        "  if isinstance(resultOutput[0:1][0], str):\n",
        "    for x in range (0, len(resultOutput)):\n",
        "      resultOutput[x:x+1] = list(map(int, resultOutput[x:x+1][0]))\n",
        "  print(type(resultOutput[0:1][0]))\n",
        "\n",
        "  print(type(resultOutput[0:1][0]))\n",
        "  return resultInput, resultOutput\n",
        "\n",
        "trainInput, trainTarget = remove_useless_outputs(newtrainInput, newtrainTarget)\n",
        "valInput, valTarget = remove_useless_outputs(newvalInput, newvalTarget)\n",
        "testInput, testTarget = remove_useless_inputs(newtestInput, newtestTarget)\n",
        "\n",
        "\n",
        "print(trainInput[0:1])\n",
        "print(len(trainInput))\n",
        "print(trainTarget[0:1])\n",
        "print(len(trainTarget))\n",
        "print(type(trainTarget[0:1]))\n"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'int'>\n",
            "<class 'int'>\n",
            "<class 'int'>\n",
            "<class 'int'>\n",
            "[[0.3287671232876711, 0.0, 0.0034439322249904397, 0.0, 0.27587890625, 0.024924571101988854, 0.0013178703215603604, 0.023927191685372606, 0.0, 0.16577420722121472, 0.15657919955906302, 0.07080194831886986, 0.06567425221342738, 0.06785172510001797, 0.01701016959638503, 0.022328109127080567, 0.02119637478718763, 7.721563054542674e-05, 0.0011800881756525248, 0.007578283773109033]]\n",
            "166448\n",
            "[0]\n",
            "166448\n",
            "<class 'list'>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7mUBGafrC5PO",
        "colab_type": "text"
      },
      "source": [
        "### Reshaping data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0PHQ90cpu0t1",
        "colab_type": "code",
        "outputId": "b7f5d1a8-f350-4db5-9897-6645e194af19",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 456
        }
      },
      "source": [
        "# Conversion to numpy array for improved memory, performance and functionality\n",
        "trainInput, trainTarget = np.array(trainInput), np.array(trainTarget)\n",
        "valInput, valTarget = np.array(valInput), np.array(valTarget)\n",
        "testInput, testTarget = np.array(testInput), np.array(testTarget)\n",
        "\n",
        "print(trainInput[0:1])\n",
        "\n",
        "trainInput = np.reshape(trainInput, (trainInput.shape[0], trainInput.shape[1], 1))\n",
        "valInput = np.reshape(valInput, (valInput.shape[0], valInput.shape[1], 1))\n",
        "testInput = np.reshape(testInput, (testInput.shape[0], testInput.shape[1], 1))\n",
        "\n",
        "print(trainInput[0:1])\n",
        "print(trainInput.shape[0])\n",
        "print(trainInput.shape[1])\n"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[3.28767123e-01 0.00000000e+00 3.44393222e-03 0.00000000e+00\n",
            "  2.75878906e-01 2.49245711e-02 1.31787032e-03 2.39271917e-02\n",
            "  0.00000000e+00 1.65774207e-01 1.56579200e-01 7.08019483e-02\n",
            "  6.56742522e-02 6.78517251e-02 1.70101696e-02 2.23281091e-02\n",
            "  2.11963748e-02 7.72156305e-05 1.18008818e-03 7.57828377e-03]]\n",
            "[[[3.28767123e-01]\n",
            "  [0.00000000e+00]\n",
            "  [3.44393222e-03]\n",
            "  [0.00000000e+00]\n",
            "  [2.75878906e-01]\n",
            "  [2.49245711e-02]\n",
            "  [1.31787032e-03]\n",
            "  [2.39271917e-02]\n",
            "  [0.00000000e+00]\n",
            "  [1.65774207e-01]\n",
            "  [1.56579200e-01]\n",
            "  [7.08019483e-02]\n",
            "  [6.56742522e-02]\n",
            "  [6.78517251e-02]\n",
            "  [1.70101696e-02]\n",
            "  [2.23281091e-02]\n",
            "  [2.11963748e-02]\n",
            "  [7.72156305e-05]\n",
            "  [1.18008818e-03]\n",
            "  [7.57828377e-03]]]\n",
            "166448\n",
            "20\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mB3brZVQZFM7",
        "colab_type": "text"
      },
      "source": [
        "# Network Implementation\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I5CxNAYlLG_S",
        "colab_type": "text"
      },
      "source": [
        "## Core Architecture"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lOVq6YexKjxU",
        "colab_type": "text"
      },
      "source": [
        "### Single Output LSTM\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mlqU_UHBkfJz",
        "colab_type": "code",
        "outputId": "a760b536-d8e7-4924-f431-49b71bc11026",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1843
        }
      },
      "source": [
        "data_dim = trainInput.shape[1]\n",
        "timesteps = trainInput.shape[0]\n",
        "\n",
        "\n",
        "# Sample Code\n",
        "# model parameters:\n",
        "\n",
        "def create_model(train_X, train_Y, data_dim):\n",
        "  lstm_units = 1024\n",
        "  \n",
        "#   print('Build baseline binary model...')\n",
        "#   model = Sequential()\n",
        "#   model.add(Masking(mask_value=0., input_shape=(data_dim, 1)))\n",
        "#   model.add(LSTM(lstm_units))\n",
        "#   model.add(Dense(1, activation='sigmoid'))\n",
        "#   model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "  \n",
        "    \n",
        "  print('Build stacked binary model')\n",
        "  lstm_units = int(lstm_units/4)\n",
        "\n",
        "  model = Sequential()\n",
        "  model.add(Masking(mask_value=0., input_shape=(data_dim, 1)))\n",
        "  model.add(LSTM(lstm_units, return_sequences=True))\n",
        "  model.add(Dropout(rate=0.2))\n",
        "  model.add(LSTM(lstm_units, return_sequences=True))\n",
        "  model.add(Dropout(rate=0.2))\n",
        "  model.add(LSTM(lstm_units, return_sequences=True))\n",
        "  model.add(Dropout(rate=0.2))\n",
        "  model.add(LSTM(lstm_units))\n",
        "  model.add(Dense(1, activation='sigmoid'))\n",
        "  model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "\n",
        "  return(model)\n",
        "\n",
        "baseline_model = create_model(trainInput, trainTarget, data_dim)\n",
        "print(baseline_model.summary())\n",
        "SVG(model_to_dot(baseline_model, show_shapes=True).create(prog='dot', format='svg'))"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING: Logging before flag parsing goes to stderr.\n",
            "W0615 23:15:13.395902 140200186578816 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
            "\n",
            "W0615 23:15:13.448407 140200186578816 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "W0615 23:15:13.484552 140200186578816 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Build stacked binary model\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "W0615 23:15:13.922070 140200186578816 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:2974: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "W0615 23:15:13.964604 140200186578816 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:133: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
            "\n",
            "W0615 23:15:13.978981 140200186578816 deprecation.py:506] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
            "W0615 23:15:15.089364 140200186578816 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n",
            "W0615 23:15:15.117688 140200186578816 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3376: The name tf.log is deprecated. Please use tf.math.log instead.\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "masking_1 (Masking)          (None, 20, 1)             0         \n",
            "_________________________________________________________________\n",
            "lstm_1 (LSTM)                (None, 20, 256)           264192    \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 20, 256)           0         \n",
            "_________________________________________________________________\n",
            "lstm_2 (LSTM)                (None, 20, 256)           525312    \n",
            "_________________________________________________________________\n",
            "dropout_2 (Dropout)          (None, 20, 256)           0         \n",
            "_________________________________________________________________\n",
            "lstm_3 (LSTM)                (None, 20, 256)           525312    \n",
            "_________________________________________________________________\n",
            "dropout_3 (Dropout)          (None, 20, 256)           0         \n",
            "_________________________________________________________________\n",
            "lstm_4 (LSTM)                (None, 256)               525312    \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 1)                 257       \n",
            "=================================================================\n",
            "Total params: 1,840,385\n",
            "Trainable params: 1,840,385\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<IPython.core.display.SVG object>"
            ],
            "image/svg+xml": "<svg height=\"792pt\" viewBox=\"0.00 0.00 310.00 792.00\" width=\"310pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n<g class=\"graph\" id=\"graph0\" transform=\"scale(1 1) rotate(0) translate(4 788)\">\n<title>G</title>\n<polygon fill=\"#ffffff\" points=\"-4,4 -4,-788 306,-788 306,4 -4,4\" stroke=\"transparent\"/>\n<!-- 140199184540000 -->\n<g class=\"node\" id=\"node1\">\n<title>140199184540000</title>\n<polygon fill=\"none\" points=\"5.5,-664.5 5.5,-710.5 296.5,-710.5 296.5,-664.5 5.5,-664.5\" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"74.5\" y=\"-683.8\">masking_1: Masking</text>\n<polyline fill=\"none\" points=\"143.5,-664.5 143.5,-710.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"172.5\" y=\"-695.3\">input:</text>\n<polyline fill=\"none\" points=\"143.5,-687.5 201.5,-687.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"172.5\" y=\"-672.3\">output:</text>\n<polyline fill=\"none\" points=\"201.5,-664.5 201.5,-710.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"249\" y=\"-695.3\">(None, 20, 1)</text>\n<polyline fill=\"none\" points=\"201.5,-687.5 296.5,-687.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"249\" y=\"-672.3\">(None, 20, 1)</text>\n</g>\n<!-- 140199184801744 -->\n<g class=\"node\" id=\"node2\">\n<title>140199184801744</title>\n<polygon fill=\"none\" points=\"16,-581.5 16,-627.5 286,-627.5 286,-581.5 16,-581.5\" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"67\" y=\"-600.8\">lstm_1: LSTM</text>\n<polyline fill=\"none\" points=\"118,-581.5 118,-627.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"147\" y=\"-612.3\">input:</text>\n<polyline fill=\"none\" points=\"118,-604.5 176,-604.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"147\" y=\"-589.3\">output:</text>\n<polyline fill=\"none\" points=\"176,-581.5 176,-627.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"231\" y=\"-612.3\">(None, 20, 1)</text>\n<polyline fill=\"none\" points=\"176,-604.5 286,-604.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"231\" y=\"-589.3\">(None, 20, 256)</text>\n</g>\n<!-- 140199184540000&#45;&gt;140199184801744 -->\n<g class=\"edge\" id=\"edge2\">\n<title>140199184540000-&gt;140199184801744</title>\n<path d=\"M151,-664.3799C151,-656.1745 151,-646.7679 151,-637.8786\" fill=\"none\" stroke=\"#000000\"/>\n<polygon fill=\"#000000\" points=\"154.5001,-637.784 151,-627.784 147.5001,-637.784 154.5001,-637.784\" stroke=\"#000000\"/>\n</g>\n<!-- 140199185380128 -->\n<g class=\"node\" id=\"node3\">\n<title>140199185380128</title>\n<polygon fill=\"none\" points=\"0,-498.5 0,-544.5 302,-544.5 302,-498.5 0,-498.5\" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"67\" y=\"-517.8\">dropout_1: Dropout</text>\n<polyline fill=\"none\" points=\"134,-498.5 134,-544.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"163\" y=\"-529.3\">input:</text>\n<polyline fill=\"none\" points=\"134,-521.5 192,-521.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"163\" y=\"-506.3\">output:</text>\n<polyline fill=\"none\" points=\"192,-498.5 192,-544.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"247\" y=\"-529.3\">(None, 20, 256)</text>\n<polyline fill=\"none\" points=\"192,-521.5 302,-521.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"247\" y=\"-506.3\">(None, 20, 256)</text>\n</g>\n<!-- 140199184801744&#45;&gt;140199185380128 -->\n<g class=\"edge\" id=\"edge3\">\n<title>140199184801744-&gt;140199185380128</title>\n<path d=\"M151,-581.3799C151,-573.1745 151,-563.7679 151,-554.8786\" fill=\"none\" stroke=\"#000000\"/>\n<polygon fill=\"#000000\" points=\"154.5001,-554.784 151,-544.784 147.5001,-554.784 154.5001,-554.784\" stroke=\"#000000\"/>\n</g>\n<!-- 140199184800176 -->\n<g class=\"node\" id=\"node4\">\n<title>140199184800176</title>\n<polygon fill=\"none\" points=\"16,-415.5 16,-461.5 286,-461.5 286,-415.5 16,-415.5\" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"67\" y=\"-434.8\">lstm_2: LSTM</text>\n<polyline fill=\"none\" points=\"118,-415.5 118,-461.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"147\" y=\"-446.3\">input:</text>\n<polyline fill=\"none\" points=\"118,-438.5 176,-438.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"147\" y=\"-423.3\">output:</text>\n<polyline fill=\"none\" points=\"176,-415.5 176,-461.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"231\" y=\"-446.3\">(None, 20, 256)</text>\n<polyline fill=\"none\" points=\"176,-438.5 286,-438.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"231\" y=\"-423.3\">(None, 20, 256)</text>\n</g>\n<!-- 140199185380128&#45;&gt;140199184800176 -->\n<g class=\"edge\" id=\"edge4\">\n<title>140199185380128-&gt;140199184800176</title>\n<path d=\"M151,-498.3799C151,-490.1745 151,-480.7679 151,-471.8786\" fill=\"none\" stroke=\"#000000\"/>\n<polygon fill=\"#000000\" points=\"154.5001,-471.784 151,-461.784 147.5001,-471.784 154.5001,-471.784\" stroke=\"#000000\"/>\n</g>\n<!-- 140198797423392 -->\n<g class=\"node\" id=\"node5\">\n<title>140198797423392</title>\n<polygon fill=\"none\" points=\"0,-332.5 0,-378.5 302,-378.5 302,-332.5 0,-332.5\" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"67\" y=\"-351.8\">dropout_2: Dropout</text>\n<polyline fill=\"none\" points=\"134,-332.5 134,-378.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"163\" y=\"-363.3\">input:</text>\n<polyline fill=\"none\" points=\"134,-355.5 192,-355.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"163\" y=\"-340.3\">output:</text>\n<polyline fill=\"none\" points=\"192,-332.5 192,-378.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"247\" y=\"-363.3\">(None, 20, 256)</text>\n<polyline fill=\"none\" points=\"192,-355.5 302,-355.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"247\" y=\"-340.3\">(None, 20, 256)</text>\n</g>\n<!-- 140199184800176&#45;&gt;140198797423392 -->\n<g class=\"edge\" id=\"edge5\">\n<title>140199184800176-&gt;140198797423392</title>\n<path d=\"M151,-415.3799C151,-407.1745 151,-397.7679 151,-388.8786\" fill=\"none\" stroke=\"#000000\"/>\n<polygon fill=\"#000000\" points=\"154.5001,-388.784 151,-378.784 147.5001,-388.784 154.5001,-388.784\" stroke=\"#000000\"/>\n</g>\n<!-- 140198845650368 -->\n<g class=\"node\" id=\"node6\">\n<title>140198845650368</title>\n<polygon fill=\"none\" points=\"16,-249.5 16,-295.5 286,-295.5 286,-249.5 16,-249.5\" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"67\" y=\"-268.8\">lstm_3: LSTM</text>\n<polyline fill=\"none\" points=\"118,-249.5 118,-295.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"147\" y=\"-280.3\">input:</text>\n<polyline fill=\"none\" points=\"118,-272.5 176,-272.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"147\" y=\"-257.3\">output:</text>\n<polyline fill=\"none\" points=\"176,-249.5 176,-295.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"231\" y=\"-280.3\">(None, 20, 256)</text>\n<polyline fill=\"none\" points=\"176,-272.5 286,-272.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"231\" y=\"-257.3\">(None, 20, 256)</text>\n</g>\n<!-- 140198797423392&#45;&gt;140198845650368 -->\n<g class=\"edge\" id=\"edge6\">\n<title>140198797423392-&gt;140198845650368</title>\n<path d=\"M151,-332.3799C151,-324.1745 151,-314.7679 151,-305.8786\" fill=\"none\" stroke=\"#000000\"/>\n<polygon fill=\"#000000\" points=\"154.5001,-305.784 151,-295.784 147.5001,-305.784 154.5001,-305.784\" stroke=\"#000000\"/>\n</g>\n<!-- 140198794738936 -->\n<g class=\"node\" id=\"node7\">\n<title>140198794738936</title>\n<polygon fill=\"none\" points=\"0,-166.5 0,-212.5 302,-212.5 302,-166.5 0,-166.5\" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"67\" y=\"-185.8\">dropout_3: Dropout</text>\n<polyline fill=\"none\" points=\"134,-166.5 134,-212.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"163\" y=\"-197.3\">input:</text>\n<polyline fill=\"none\" points=\"134,-189.5 192,-189.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"163\" y=\"-174.3\">output:</text>\n<polyline fill=\"none\" points=\"192,-166.5 192,-212.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"247\" y=\"-197.3\">(None, 20, 256)</text>\n<polyline fill=\"none\" points=\"192,-189.5 302,-189.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"247\" y=\"-174.3\">(None, 20, 256)</text>\n</g>\n<!-- 140198845650368&#45;&gt;140198794738936 -->\n<g class=\"edge\" id=\"edge7\">\n<title>140198845650368-&gt;140198794738936</title>\n<path d=\"M151,-249.3799C151,-241.1745 151,-231.7679 151,-222.8786\" fill=\"none\" stroke=\"#000000\"/>\n<polygon fill=\"#000000\" points=\"154.5001,-222.784 151,-212.784 147.5001,-222.784 154.5001,-222.784\" stroke=\"#000000\"/>\n</g>\n<!-- 140198881854240 -->\n<g class=\"node\" id=\"node8\">\n<title>140198881854240</title>\n<polygon fill=\"none\" points=\"16,-83.5 16,-129.5 286,-129.5 286,-83.5 16,-83.5\" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"67\" y=\"-102.8\">lstm_4: LSTM</text>\n<polyline fill=\"none\" points=\"118,-83.5 118,-129.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"147\" y=\"-114.3\">input:</text>\n<polyline fill=\"none\" points=\"118,-106.5 176,-106.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"147\" y=\"-91.3\">output:</text>\n<polyline fill=\"none\" points=\"176,-83.5 176,-129.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"231\" y=\"-114.3\">(None, 20, 256)</text>\n<polyline fill=\"none\" points=\"176,-106.5 286,-106.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"231\" y=\"-91.3\">(None, 256)</text>\n</g>\n<!-- 140198794738936&#45;&gt;140198881854240 -->\n<g class=\"edge\" id=\"edge8\">\n<title>140198794738936-&gt;140198881854240</title>\n<path d=\"M151,-166.3799C151,-158.1745 151,-148.7679 151,-139.8786\" fill=\"none\" stroke=\"#000000\"/>\n<polygon fill=\"#000000\" points=\"154.5001,-139.784 151,-129.784 147.5001,-139.784 154.5001,-139.784\" stroke=\"#000000\"/>\n</g>\n<!-- 140198787419384 -->\n<g class=\"node\" id=\"node9\">\n<title>140198787419384</title>\n<polygon fill=\"none\" points=\"25,-.5 25,-46.5 277,-46.5 277,-.5 25,-.5\" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"78.5\" y=\"-19.8\">dense_1: Dense</text>\n<polyline fill=\"none\" points=\"132,-.5 132,-46.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"161\" y=\"-31.3\">input:</text>\n<polyline fill=\"none\" points=\"132,-23.5 190,-23.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"161\" y=\"-8.3\">output:</text>\n<polyline fill=\"none\" points=\"190,-.5 190,-46.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"233.5\" y=\"-31.3\">(None, 256)</text>\n<polyline fill=\"none\" points=\"190,-23.5 277,-23.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"233.5\" y=\"-8.3\">(None, 1)</text>\n</g>\n<!-- 140198881854240&#45;&gt;140198787419384 -->\n<g class=\"edge\" id=\"edge9\">\n<title>140198881854240-&gt;140198787419384</title>\n<path d=\"M151,-83.3799C151,-75.1745 151,-65.7679 151,-56.8786\" fill=\"none\" stroke=\"#000000\"/>\n<polygon fill=\"#000000\" points=\"154.5001,-56.784 151,-46.784 147.5001,-56.784 154.5001,-56.784\" stroke=\"#000000\"/>\n</g>\n<!-- 140199184800512 -->\n<g class=\"node\" id=\"node10\">\n<title>140199184800512</title>\n<polygon fill=\"none\" points=\"86.5,-747.5 86.5,-783.5 215.5,-783.5 215.5,-747.5 86.5,-747.5\" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"151\" y=\"-761.8\">140199184800512</text>\n</g>\n<!-- 140199184800512&#45;&gt;140199184540000 -->\n<g class=\"edge\" id=\"edge1\">\n<title>140199184800512-&gt;140199184540000</title>\n<path d=\"M151,-747.4092C151,-739.4308 151,-729.795 151,-720.606\" fill=\"none\" stroke=\"#000000\"/>\n<polygon fill=\"#000000\" points=\"154.5001,-720.5333 151,-710.5333 147.5001,-720.5334 154.5001,-720.5333\" stroke=\"#000000\"/>\n</g>\n</g>\n</svg>"
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JXmP9FbSJ6D0",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 361
        },
        "outputId": "94f3d7a1-0786-4c6f-ccf8-72b529315b06"
      },
      "source": [
        "baseline_model.fit(trainInput, trainTarget, epochs = 30, batch_size = 1024, verbose = 1)"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/30\n",
            "166448/166448 [==============================] - 990s 6ms/step - loss: 0.5107 - acc: 0.7444\n",
            "Epoch 2/30\n",
            " 93184/166448 [===============>..............] - ETA: 7:15 - loss: 0.5094 - acc: 0.7450"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-26-cac413baf96b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mbaseline_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainInput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainTarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m30\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1024\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1037\u001b[0m                                         \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1038\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1039\u001b[0;31m                                         validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1040\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1041\u001b[0m     def evaluate(self, x=None, y=None,\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[0;34m(model, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m    197\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 199\u001b[0;31m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    200\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2713\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2714\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2715\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2716\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2717\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2673\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2674\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2675\u001b[0;31m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2676\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1456\u001b[0m         ret = tf_session.TF_SessionRunCallable(self._session._session,\n\u001b[1;32m   1457\u001b[0m                                                \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1458\u001b[0;31m                                                run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1459\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1460\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mvSsHFJZ2iV5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "baseline_model.save('baseline.h5') "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FjqbxaxyTwaf",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        },
        "outputId": "c1225d9f-e419-4ae3-f6fe-5f9ac76dc5c4"
      },
      "source": [
        "_25\n"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f829abe3390>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6_oxovoRVHWL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "new = load_model('baseline.h5')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XDbr0MxLZWCd",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 131
        },
        "outputId": "d9dcfbc3-b752-4c0c-ab06-489b5c330d36"
      },
      "source": [
        "pred = baseline_model.predict(testInput)\n",
        "print(pred)"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[0.01071534]\n",
            " [0.8214457 ]\n",
            " [0.89062285]\n",
            " ...\n",
            " [0.9633812 ]\n",
            " [0.70384765]\n",
            " [0.86349875]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "23zEXEg7Zk1W",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 131
        },
        "outputId": "8d5faace-2973-4720-8741-c62d59d0d68d"
      },
      "source": [
        "pred = new.predict(testInput)\n",
        "print(pred)"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[0.01071534]\n",
            " [0.8214457 ]\n",
            " [0.89062285]\n",
            " ...\n",
            " [0.9633812 ]\n",
            " [0.70384765]\n",
            " [0.86349875]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H1QuNzDbbCg0",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        },
        "outputId": "6e5f2ad2-fa19-491e-b52a-879e732f6bea"
      },
      "source": [
        "newpred = new.predict_classes(testInput, verbose=1)"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "49052/49052 [==============================] - 182s 4ms/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WfRZ_DJ4bnJm",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        },
        "outputId": "8187e78b-9bee-46dd-e29f-df78d0154567"
      },
      "source": [
        "print (type(newpred))"
      ],
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'numpy.ndarray'>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4NcEC7mQWIB_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 337
        },
        "outputId": "ea486e8d-c2c6-4d5e-99b9-c2ed21ec0ad7"
      },
      "source": [
        "def get_predict_and_score(model, X, Y):\n",
        "    # transform the prediction to the original scale.\n",
        "    # transform also the label to the original scale for interpretability.\n",
        "    # calculate RMSE.\n",
        "    score = sqrt(mean_squared_error(X[0], Y[:, 0]))\n",
        "    return(score, pred)\n",
        "\n",
        "mse_train, train_predict = get_predict_and_score(baseline_model, trainInput, trainTarget)\n",
        "mse_test, test_predict = get_predict_and_score(baseline_model, testInput, testTarget)\n",
        "\n",
        "print(\"Training data error: %.2f MSE\" % mse_train)\n",
        "print(\"Test data error: %.2f MSE\" % mse_test)"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-39-287e58eb7237>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;32mreturn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscore\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mmse_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_predict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_predict_and_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbaseline_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainInput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainTarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0mmse_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_predict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_predict_and_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbaseline_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtestInput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtestTarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-39-287e58eb7237>\u001b[0m in \u001b[0;36mget_predict_and_score\u001b[0;34m(model, X, Y)\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0;31m# transform also the label to the original scale for interpretability.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;31m# calculate RMSE.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0mscore\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmean_squared_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0;32mreturn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscore\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'sqrt' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I6lAccHrKrx3",
        "colab_type": "text"
      },
      "source": [
        "### Multi output LSTM\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VbhiWb-0NDdx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data_dim = trainInput.shape[1]\n",
        "timesteps = trainInput.shape[0]\n",
        "\n",
        "class fypNet: \n",
        "  @staticmethod\n",
        "  def build_input_predictor(inputs, data_dim, lstm_units):\n",
        "    inPred = Masking(mask_value=0., input_shape = (data_dim, 1))(inputs)\n",
        "    inPred = LSTM(lstm_units, return_sequences=True)(inPred)\n",
        "    \n",
        "    inPred = Dense(1)(inPred)\n",
        "    result = Activation('softmax', name= 'inPred_result')(inPred)\n",
        "    \n",
        "    return result\n",
        "    \n",
        "  @staticmethod\n",
        "  def build_output_predictor(inputs, data_dim, lstm_units): \n",
        "    outPred = LSTM(lstm_units, return_sequences=True)(inputs)\n",
        "    outPred = LSTM(lstm_units)(outPred)\n",
        "    \n",
        "    outPred = Dense(1)(outPred)\n",
        "    result = Activation('sigmoid', name= 'outPred_result')(outPred)\n",
        "\n",
        "    return result\n",
        "  \n",
        "  @staticmethod\n",
        "  def build(data_dim, lstm_units):\n",
        "         \n",
        "    input_shape = (data_dim, 1)\n",
        "    inputs = Input(shape = input_shape)\n",
        "   \n",
        "    inputBranch = fypNet.build_input_predictor(inputs, data_dim, lstm_units)\n",
        "    outputBranch = fypNet.build_output_predictor(inputs, data_dim, lstm_units)\n",
        "    \n",
        "    model = Model(inputs= inputs, outputs= [inputBranch, outputBranch])\n",
        "    \n",
        "    return model\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tW6yVBGPUrmk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data_dim = trainInput.shape[1]\n",
        "timesteps = trainInput.shape[0]\n",
        "lstm_units = 1024\n",
        "num_epochs = 10\n",
        "initial_lr = 1e-3\n",
        "batch_size = 32\n",
        "\n",
        "# initialize our fypNet multi-output network\n",
        "model = fypNet.build(data_dim, lstm_units)\n",
        " \n",
        "losses = {'inPred_result': 'mean_absolute_error','outPred_result': 'binary_crossentropy'}\n",
        " \n",
        "# initialize the optimizer and compile the model\n",
        "\n",
        "print('Compiling model...')\n",
        "# opt = Adam(lr=initial_lr, decay=initial_lr/epochs)\n",
        "model.compile(optimizer='adam', loss=losses, metrics=['acc'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fiNO04Z3i1cz",
        "colab_type": "code",
        "outputId": "a3b4d0af-8707-48e3-d247-30fe8fa62cd6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 433
        }
      },
      "source": [
        "# inPred = trainInput.shift(-1)\n",
        "trainInTarget = np.roll(trainInput, -1)\n",
        "trainValTarget = np.roll(valInput, -1)\n",
        "\n",
        "# print(inPred.head())\n",
        "print(model.summary())\n",
        "SVG(model_to_dot(model, show_shapes=True).create(prog='dot', format='svg'))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_2 (InputLayer)            (None, 20, 1)        0                                            \n",
            "__________________________________________________________________________________________________\n",
            "masking_3 (Masking)             (None, 20, 1)        0           input_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lstm_8 (LSTM)                   (None, 20, 128)      66560       input_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lstm_7 (LSTM)                   (None, 20, 128)      66560       masking_3[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "lstm_9 (LSTM)                   (None, 128)          131584      lstm_8[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "dense_4 (Dense)                 (None, 20, 1)        129         lstm_7[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "dense_5 (Dense)                 (None, 1)            129         lstm_9[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "inPred_result (Activation)      (None, 20, 1)        0           dense_4[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "outPred_result (Activation)     (None, 1)            0           dense_5[0][0]                    \n",
            "==================================================================================================\n",
            "Total params: 264,962\n",
            "Trainable params: 264,962\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MYXPMKKzL1eL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.fit(trainInput,{\"inPred_result\": trainInTarget, \"outPred_result\": trainTarget}, validation_data=(valInput, {\"inPred_result\": trainValTarget, \"outPred_result\": valTarget}), batch_size = batch_size, Stateful=False, epochs=num_epochs, verbose=1)\n",
        "# Input prediction is the input vector shifted by 1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LdfWnWD_Ltfs",
        "colab_type": "text"
      },
      "source": [
        "# Testing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ivAgZyBQ_y_a",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yBvqxci1kVZn",
        "colab_type": "text"
      },
      "source": [
        "# Benchmark\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5nVbj_6GUacG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}