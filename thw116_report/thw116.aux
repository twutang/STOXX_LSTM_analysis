\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\@writefile{toc}{\contentsline {section}{\numberline {1}\hskip -1em.\nobreakspace  {}Introduction}{4}{section.1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.1}\hskip -1em.\nobreakspace  {}Project Motivation}{4}{subsection.1.1}}
\newlabel{Project Motivation}{{1.1}{4}{\hskip -1em.~Project Motivation}{subsection.1.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.2}\hskip -1em.\nobreakspace  {}Problem Definition}{4}{subsection.1.2}}
\newlabel{Problem Definition}{{1.2}{4}{\hskip -1em.~Problem Definition}{subsection.1.2}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {1.2.1}Intended Deliverables}{4}{subsubsection.1.2.1}}
\newlabel{Intended Deliverables}{{1.2.1}{4}{Intended Deliverables}{subsubsection.1.2.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.3}\hskip -1em.\nobreakspace  {}Importance in Industry}{4}{subsection.1.3}}
\citation{jupyter_notebook}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.4}\hskip -1em.\nobreakspace  {}Software Usage}{5}{subsection.1.4}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.5}\hskip -1em.\nobreakspace  {}Report Structure}{5}{subsection.1.5}}
\newlabel{Report Structure}{{1.5}{5}{\hskip -1em.~Report Structure}{subsection.1.5}{}}
\citation{Rouwenhorst}
\citation{Graham}
\citation{Treynor}
\citation{Ross}
\citation{Fama}
\citation{Eun}
\citation{Merz}
\citation{Rumelhart}
\@writefile{toc}{\contentsline {section}{\numberline {2}\hskip -1em.\nobreakspace  {}Background}{6}{section.2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1}\hskip -1em.\nobreakspace  {}History of Investing Methodologies}{6}{subsection.2.1}}
\newlabel{History of Investing Methodologies}{{2.1}{6}{\hskip -1em.~History of Investing Methodologies}{subsection.2.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2}\hskip -1em.\nobreakspace  {}Long Short-Term Memory Networks}{7}{subsection.2.2}}
\newlabel{lstm}{{2.2}{7}{\hskip -1em.~Long Short-Term Memory Networks}{subsection.2.2}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.2.1}Overview}{7}{subsubsection.2.2.1}}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces A classical MLP (LHS) simply generates an output with respect to its input whereas in the RNN (RHS), the output is governed by the input and previous input.\relax }}{7}{figure.caption.3}}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces Numerous varying topologies for specific sequential data tasks.\relax }}{7}{figure.caption.4}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.2.2}The Exploding and Vanishing Gradient Problem in RNNs}{7}{subsubsection.2.2.2}}
\citation{Hockreiter}
\citation{Olah}
\citation{Olah}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces Breakdown of a LSTM cell depicting the input, forget and update gates.\relax }}{8}{figure.caption.10}}
\citation{Pascanu}
\citation{Reimers}
\citation{Janocha}
\citation{Bergmeir}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.2.3}Loss Functions}{9}{subsubsection.2.2.3}}
\newlabel{loss functions}{{2.2.3}{9}{Loss Functions}{subsubsection.2.2.3}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces Graph showing the binary cross entropy loss function given the probability of a correct prediction\relax }}{9}{figure.caption.11}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.2.4}Cross Validation}{9}{subsubsection.2.2.4}}
\newlabel{cross validation}{{2.2.4}{9}{Cross Validation}{subsubsection.2.2.4}{}}
\citation{Racine}
\citation{Gregor}
\citation{Man_Group}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.2.5}Applications}{10}{subsubsection.2.2.5}}
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces LHS: DRAW network from Google Deepmind, featuring an attention mechanism formed by an encoding and decoding RNN (with the LSTM architecture). RHS: Simulation of how each iteration improves on the previous when using the network.\relax }}{10}{figure.caption.12}}
\citation{Ahmadi}
\@writefile{lof}{\contentsline {figure}{\numberline {6}{\ignorespaces LHS: Single digits generated from the MNIST database, with the right-most column depicting original images from the database. RHS: DRAW's attempts at generating double digit values.\relax }}{11}{figure.caption.13}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3}\hskip -1em.\nobreakspace  {}Review of Machine Learning Research in Investing}{11}{subsection.2.3}}
\newlabel{ML_research}{{2.3}{11}{\hskip -1em.~Review of Machine Learning Research in Investing}{subsection.2.3}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.3.1}Overview}{11}{subsubsection.2.3.1}}
\citation{Kim}
\citation{Alberg}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.3.2}Initial Usage}{12}{subsubsection.2.3.2}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.3.3}Formalisation of Experimentation}{12}{subsubsection.2.3.3}}
\@writefile{lof}{\contentsline {figure}{\numberline {7}{\ignorespaces Kim's results of SVM compared to BP and CBR.\relax }}{12}{figure.caption.16}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.3.4}Methodologies}{12}{subsubsection.2.3.4}}
\newlabel{Alberg}{{2.3.4}{12}{Methodologies}{subsubsection.2.3.4}{}}
\citation{Makridakis}
\@writefile{lof}{\contentsline {figure}{\numberline {8}{\ignorespaces (LHS) The overview of how the model functions. (RHS) Backtesting results from Alberg with various factor models between 2000-2016. Clairvoyance x-axis equates to the time period difference between the current and future time in the model. \relax }}{13}{figure.caption.17}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.3.5}Related Works}{13}{subsubsection.2.3.5}}
\citation{Smyl}
\citation{Winters}
\citation{Henrique}
\@writefile{lof}{\contentsline {figure}{\numberline {9}{\ignorespaces LHS: Different architectures that Smyl used with respect to time horizon of data. RHS: General structure of 3 layered dilated RNN.\relax }}{14}{figure.caption.18}}
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{chang_architecture}{{9}{14}{LHS: Different architectures that Smyl used with respect to time horizon of data. RHS: General structure of 3 layered dilated RNN.\relax }{figure.caption.18}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.3.6}Summary}{14}{subsubsection.2.3.6}}
\@writefile{toc}{\contentsline {section}{\numberline {3}\hskip -1em.\nobreakspace  {}Requirements Capture}{16}{section.3}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1}\hskip -1em.\nobreakspace  {}Tasks}{16}{subsection.3.1}}
\newlabel{tasks}{{3.1}{16}{\hskip -1em.~Tasks}{subsection.3.1}{}}
\@writefile{toc}{\contentsline {section}{\numberline {4}\hskip -1em.\nobreakspace  {}Design and Analysis}{17}{section.4}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1}\hskip -1em.\nobreakspace  {}Overview}{17}{subsection.4.1}}
\@writefile{lof}{\contentsline {figure}{\numberline {10}{\ignorespaces High-level overview of the tool pipeline design\relax }}{17}{figure.caption.19}}
\newlabel{pipeline_design}{{10}{17}{High-level overview of the tool pipeline design\relax }{figure.caption.19}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {11}{\ignorespaces Overview of the processes involved in iterative designing and testing of models for the problem\relax }}{17}{figure.caption.20}}
\newlabel{process_overview}{{11}{17}{Overview of the processes involved in iterative designing and testing of models for the problem\relax }{figure.caption.20}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2}\hskip -1em.\nobreakspace  {}Data Structuring}{17}{subsection.4.2}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.2.1}Data Description}{17}{subsubsection.4.2.1}}
\newlabel{data_description}{{4.2.1}{17}{Data Description}{subsubsection.4.2.1}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {12}{\ignorespaces Diagram overview of the database hierarchy.\relax }}{18}{figure.caption.21}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.2.2}Data Formatting}{18}{subsubsection.4.2.2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.3}\hskip -1em.\nobreakspace  {}Data Preprocessing}{18}{subsection.4.3}}
\newlabel{data preprocess}{{4.3}{18}{\hskip -1em.~Data Preprocessing}{subsection.4.3}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.3.1}Data Transformation}{18}{subsubsection.4.3.1}}
\newlabel{data transformation}{{4.3.1}{18}{Data Transformation}{subsubsection.4.3.1}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {13}{\ignorespaces Histogram plots of returns (LHS) and log returns (RHS) distributions.\relax }}{19}{figure.caption.22}}
\newlabel{returns}{{13}{19}{Histogram plots of returns (LHS) and log returns (RHS) distributions.\relax }{figure.caption.22}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.3.2}Sparsity}{19}{subsubsection.4.3.2}}
\newlabel{data_analysis}{{4.3.2}{19}{Sparsity}{subsubsection.4.3.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {14}{\ignorespaces Box plot indicating the approximate average underlying count numbers for each type of input. Green triangles indicate the mean counts.\relax }}{19}{figure.caption.23}}
\newlabel{input_counts}{{14}{19}{Box plot indicating the approximate average underlying count numbers for each type of input. Green triangles indicate the mean counts.\relax }{figure.caption.23}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {15}{\ignorespaces LHS: Individual ticker breakdown of datapoint counts. RHS: Boxplot of returns output count.\relax }}{20}{figure.caption.24}}
\@writefile{lof}{\contentsline {figure}{\numberline {16}{\ignorespaces Box plot indicating the approximate average underlying delta counts for each type of input. Green triangles indicate the mean counts. As seen, LTM is particularly lower than the others.\relax }}{20}{figure.caption.25}}
\newlabel{input_delta_counts}{{16}{20}{Box plot indicating the approximate average underlying delta counts for each type of input. Green triangles indicate the mean counts. As seen, LTM is particularly lower than the others.\relax }{figure.caption.25}{}}
\@writefile{lot}{\contentsline {table}{\numberline {1}{\ignorespaces Percentages of used and complete input vectors with respect to the entire dataset.\relax }}{20}{table.caption.27}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.4}\hskip -1em.\nobreakspace  {}Architectural Design}{20}{subsection.4.4}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.4.1}Overview}{20}{subsubsection.4.4.1}}
\@writefile{lof}{\contentsline {figure}{\numberline {17}{\ignorespaces Heatmaps of nan values (depicted in white) of a few datasets.\relax }}{21}{figure.caption.26}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.4.2}Inputs and Outputs}{21}{subsubsection.4.4.2}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.4.3}Loss Function}{22}{subsubsection.4.4.3}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.4.4}Design Iterations}{22}{subsubsection.4.4.4}}
\newlabel{design iterations}{{4.4.4}{22}{Design Iterations}{subsubsection.4.4.4}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {18}{\ignorespaces LHS: Architectural overview of the initial one hidden layer model. RHS: Model summary with number of parameters.\relax }}{22}{figure.caption.28}}
\@writefile{lof}{\contentsline {figure}{\numberline {19}{\ignorespaces LHS: Architectural overview of the stacked LSTM model. RHS: Model summary with number of parameters.\relax }}{23}{figure.caption.29}}
\newlabel{stacked_lstm}{{19}{23}{LHS: Architectural overview of the stacked LSTM model. RHS: Model summary with number of parameters.\relax }{figure.caption.29}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {20}{\ignorespaces Demonstration of errors introduced into output function of LSTM by missing values.\relax }}{23}{figure.caption.30}}
\citation{Ruder}
\@writefile{lof}{\contentsline {figure}{\numberline {21}{\ignorespaces Overview of the architecture of the multi output model.\relax }}{24}{figure.caption.31}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.5}\hskip -1em.\nobreakspace  {}Training and Testing}{24}{subsection.4.5}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.6}\hskip -1em.\nobreakspace  {}Hyperparameter Optimisation}{24}{subsection.4.6}}
\newlabel{hyperparameters}{{4.6}{24}{\hskip -1em.~Hyperparameter Optimisation}{subsection.4.6}{}}
\citation{Adam}
\citation{brownlee_activation}
\citation{Gal}
\citation{Sak}
\citation{Hagan}
\citation{DeMiguel}
\@writefile{lof}{\contentsline {figure}{\numberline {22}{\ignorespaces Screenshot indicating an issue with no decreasing loss function during training due to an incorrectly selected activation function.\relax }}{25}{figure.caption.32}}
\citation{Sharpe}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.7}\hskip -1em.\nobreakspace  {}Benchmark Design}{26}{subsection.4.7}}
\newlabel{benchmark}{{4.7}{26}{\hskip -1em.~Benchmark Design}{subsection.4.7}{}}
\@writefile{toc}{\contentsline {section}{\numberline {5}\hskip -1em.\nobreakspace  {}Implementation}{27}{section.5}}
\newlabel{implementation}{{5}{27}{\hskip -1em.~Implementation}{section.5}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.1}\hskip -1em.\nobreakspace  {}Data Structuring}{27}{subsection.5.1}}
\@writefile{lof}{\contentsline {figure}{\numberline {23}{\ignorespaces Program flow diagram of data structuring processes.\relax }}{27}{figure.caption.33}}
\newlabel{data_structuring}{{23}{27}{Program flow diagram of data structuring processes.\relax }{figure.caption.33}{}}
\newlabel{file_mount}{{1}{27}{Importing csv file to dataframe}{lstlisting.1}{}}
\@writefile{lol}{\contentsline {lstlisting}{\numberline {1}Importing csv file to dataframe.}{27}{lstlisting.1}}
\@writefile{lol}{\contentsline {lstlisting}{\numberline {2}Formation of lists of dataframes according to type of input.}{27}{lstlisting.2}}
\newlabel{datetime_index}{{3}{27}{Function to create datetime index for dataframes}{lstlisting.3}{}}
\@writefile{lol}{\contentsline {lstlisting}{\numberline {3}Function to create datetime index for dataframes.}{27}{lstlisting.3}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.2}\hskip -1em.\nobreakspace  {}Data Preprocessing}{28}{subsection.5.2}}
\@writefile{lof}{\contentsline {figure}{\numberline {24}{\ignorespaces Program flow diagram of data preprocessing processes.\relax }}{28}{figure.caption.34}}
\newlabel{data_preprocessing}{{24}{28}{Program flow diagram of data preprocessing processes.\relax }{figure.caption.34}{}}
\newlabel{datachange}{{4}{28}{Identifying the available datapoint counts in differences of $x_{t}$ and $x_{t-1}$}{lstlisting.4}{}}
\@writefile{lol}{\contentsline {lstlisting}{\numberline {4}Identifying the available datapoint counts in differences of $x_{t}$ and $x_{t-1}$.}{28}{lstlisting.4}}
\@writefile{lol}{\contentsline {lstlisting}{\numberline {5}Functionality to split original entire dataset into allocated training split size.}{29}{lstlisting.5}}
\@writefile{lol}{\contentsline {lstlisting}{\numberline {6}Functionality to scale the input attributes between 0 and 1.}{29}{lstlisting.6}}
\@writefile{lol}{\contentsline {lstlisting}{\numberline {7}Function to convert the raw output target to a binary label.}{30}{lstlisting.7}}
\@writefile{lol}{\contentsline {lstlisting}{\numberline {8}Formation of the input vectors for the model.}{30}{lstlisting.8}}
\@writefile{lof}{\contentsline {figure}{\numberline {25}{\ignorespaces Diagram of reshaped input data structure for Keras.\relax }}{31}{figure.caption.35}}
\@writefile{lol}{\contentsline {lstlisting}{\numberline {9}Reshaping the input \textit  {numpy} arrays to the correct format.}{32}{lstlisting.9}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.3}\hskip -1em.\nobreakspace  {}Architecture}{32}{subsection.5.3}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {5.3.1}Single Output Model}{32}{subsubsection.5.3.1}}
\@writefile{lol}{\contentsline {lstlisting}{\numberline {10}Model creation, compilation and fitting functionality of the single output model. The initial single hidden layer model is commented out.}{32}{lstlisting.10}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {5.3.2}Multi output Model}{33}{subsubsection.5.3.2}}
\@writefile{lol}{\contentsline {lstlisting}{\numberline {11}Structuring of the multi output model, with two branches focused on the separate tasks of predicting the target variable, $y$ and the next input vector, $x_{t+1}$.}{33}{lstlisting.11}}
\@writefile{lol}{\contentsline {lstlisting}{\numberline {12}Initialisation, compilation and training implementation for the multi output model}{34}{lstlisting.12}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.4}\hskip -1em.\nobreakspace  {}Evaluation Metrics}{35}{subsection.5.4}}
\@writefile{lof}{\contentsline {figure}{\numberline {26}{\ignorespaces Program flow diagram of evaluation processes.\relax }}{35}{figure.caption.36}}
\newlabel{evaluationflow}{{26}{35}{Program flow diagram of evaluation processes.\relax }{figure.caption.36}{}}
\@writefile{lol}{\contentsline {lstlisting}{\numberline {13}Keras predict functions to generate predicted labels from test data.}{35}{lstlisting.13}}
\@writefile{lof}{\contentsline {figure}{\numberline {27}{\ignorespaces Confusion matrix for labelling prediction results.\relax }}{36}{figure.caption.37}}
\newlabel{confusion_matrix}{{27}{36}{Confusion matrix for labelling prediction results.\relax }{figure.caption.37}{}}
\@writefile{lol}{\contentsline {lstlisting}{\numberline {14}Implemented function which evaluates the model prediction performance based on accuracy\tmspace  +\thinmuskip {.1667em} precision and recall.}{36}{lstlisting.14}}
\@writefile{lol}{\contentsline {lstlisting}{\numberline {15}Filtering out positively labelled predictions to extract related actual return.}{36}{lstlisting.15}}
\citation{python_testing}
\@writefile{toc}{\contentsline {section}{\numberline {6}\hskip -1em.\nobreakspace  {}Testing}{38}{section.6}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.1}\hskip -1em.\nobreakspace  {}Test Plan}{38}{subsection.6.1}}
\@writefile{lof}{\contentsline {figure}{\numberline {28}{\ignorespaces Levels of software testing from high (Top) to low (Bottom)\relax }}{38}{figure.caption.38}}
\@writefile{lol}{\contentsline {lstlisting}{\numberline {16}Example Test.}{38}{lstlisting.16}}
\@writefile{lof}{\contentsline {figure}{\numberline {29}{\ignorespaces Flow diagram of how an integrated test may be executed.\relax }}{38}{figure.caption.39}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.2}\hskip -1em.\nobreakspace  {}Unit Test Implementation}{39}{subsection.6.2}}
\@writefile{lol}{\contentsline {lstlisting}{\numberline {17}Example of a DataFrame comparison test.}{39}{lstlisting.17}}
\newlabel{error_message}{{18}{39}{Assertion Error raised by the second fail case of the Test Set Generator Test}{lstlisting.18}{}}
\@writefile{lol}{\contentsline {lstlisting}{\numberline {18}Assertion Error raised by the second fail case of the Test Set Generator Test.}{39}{lstlisting.18}}
\@writefile{lol}{\contentsline {lstlisting}{\numberline {19}Example of a \textit  {numpy} comparison test.}{39}{lstlisting.19}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.3}\hskip -1em.\nobreakspace  {}Integration Test Implementation}{40}{subsection.6.3}}
\@writefile{lof}{\contentsline {figure}{\numberline {30}{\ignorespaces Integration testing may be more useful in the contrived bottom program flow example where functionality is not very linear. This project's program flow is more similar to that of the top program flow, depicted in blue.\relax }}{40}{figure.caption.40}}
\newlabel{integration_useful}{{30}{40}{Integration testing may be more useful in the contrived bottom program flow example where functionality is not very linear. This project's program flow is more similar to that of the top program flow, depicted in blue.\relax }{figure.caption.40}{}}
\newlabel{integration_test}{{20}{40}{Example of an integration test for 3 linked functionalities}{lstlisting.20}{}}
\@writefile{lol}{\contentsline {lstlisting}{\numberline {20}Example of an integration test for 3 linked functionalities.}{40}{lstlisting.20}}
\@writefile{toc}{\contentsline {section}{\numberline {7}\hskip -1em.\nobreakspace  {}Results}{42}{section.7}}
\newlabel{results}{{7}{42}{\hskip -1em.~Results}{section.7}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {31}{\ignorespaces LHS: Plot of multi output model's losses for next input predictor. RHS: Plot of multi output model's losses for target variable predictor.\relax }}{42}{figure.caption.41}}
\newlabel{loss_plots}{{31}{42}{LHS: Plot of multi output model's losses for next input predictor. RHS: Plot of multi output model's losses for target variable predictor.\relax }{figure.caption.41}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {32}{\ignorespaces LHS: Plot of multi output model's accuracy for next input predictor. RHS: Plot of multi output model's accuracy for target variable predictor.\relax }}{42}{figure.caption.42}}
\newlabel{accuracy_plots}{{32}{42}{LHS: Plot of multi output model's accuracy for next input predictor. RHS: Plot of multi output model's accuracy for target variable predictor.\relax }{figure.caption.42}{}}
\citation{google}
\newlabel{libraries}{{21}{43}{New proposed implementation of the optimiser}{lstlisting.21}{}}
\@writefile{lol}{\contentsline {lstlisting}{\numberline {21}New proposed implementation of the optimiser.}{43}{lstlisting.21}}
\@writefile{lot}{\contentsline {table}{\numberline {2}{\ignorespaces Evaluation metrics of the model performed on hold out test data.\relax }}{43}{table.caption.44}}
\newlabel{eval_metrics}{{2}{43}{Evaluation metrics of the model performed on hold out test data.\relax }{table.caption.44}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {33}{\ignorespaces Top: Initial classification threshold resulting in Precision = 0.8, Recall = 0.73. Bottom: Higher classification threshold with results in Precision = 0.88, Recall = 0.64.\relax }}{44}{figure.caption.43}}
\newlabel{classification_threshold}{{33}{44}{Top: Initial classification threshold resulting in Precision = 0.8, Recall = 0.73. Bottom: Higher classification threshold with results in Precision = 0.88, Recall = 0.64.\relax }{figure.caption.43}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {34}{\ignorespaces Histogram of the returns and log returns of the selected labels by the model.\relax }}{44}{figure.caption.45}}
\@writefile{lof}{\contentsline {figure}{\numberline {35}{\ignorespaces Histogram of the returns and log returns of the entire unseen test data set.\relax }}{45}{figure.caption.46}}
\@writefile{lot}{\contentsline {table}{\numberline {3}{\ignorespaces Statistical summary of percentage changes i.e. returns of sets of data. Entire Dataset equates to all returns from 2000 onwards, Test Data is the returns allocated to the test split and Model Selected Data is the collection of returns that the model predicted would have positive returns from the Test Dataset.\relax }}{45}{table.caption.47}}
\newlabel{benchmarkstats}{{3}{45}{Statistical summary of percentage changes i.e. returns of sets of data. Entire Dataset equates to all returns from 2000 onwards, Test Data is the returns allocated to the test split and Model Selected Data is the collection of returns that the model predicted would have positive returns from the Test Dataset.\relax }{table.caption.47}{}}
\@writefile{toc}{\contentsline {section}{\numberline {8}\hskip -1em.\nobreakspace  {}Conclusions}{46}{section.8}}
\@writefile{toc}{\contentsline {subsection}{\numberline {8.1}\hskip -1em.\nobreakspace  {}Objectives Completion}{46}{subsection.8.1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {8.2}\hskip -1em.\nobreakspace  {}Evaluation}{46}{subsection.8.2}}
\newlabel{evaluation}{{8.2}{46}{\hskip -1em.~Evaluation}{subsection.8.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {8.3}\hskip -1em.\nobreakspace  {}Future Work}{46}{subsection.8.3}}
\@writefile{lof}{\contentsline {figure}{\numberline {36}{\ignorespaces Feature selection and forecast models by Uber.\relax }}{47}{figure.caption.48}}
\bibstyle{ieee}
\bibdata{thw116bib}
\bibcite{jupyter_notebook}{1}
\bibcite{Ahmadi}{2}
\bibcite{Alberg}{3}
\bibcite{Bergmeir}{4}
\bibcite{brownlee_activation}{5}
\bibcite{DeMiguel}{6}
\bibcite{Eun}{7}
\bibcite{Fama}{8}
\bibcite{Gal}{9}
\bibcite{google}{10}
\bibcite{Graham}{11}
\bibcite{Gregor}{12}
\bibcite{Hagan}{13}
\bibcite{Henrique}{14}
\bibcite{Hockreiter}{15}
\bibcite{Kim}{16}
\bibcite{Janocha}{17}
\bibcite{Adam}{18}
\bibcite{Makridakis}{19}
\bibcite{Merz}{20}
\bibcite{Olah}{21}
\bibcite{Pascanu}{22}
\bibcite{Racine}{23}
\bibcite{Reimers}{24}
\bibcite{Man_Group}{25}
\bibcite{Ross}{26}
\bibcite{Rouwenhorst}{27}
\bibcite{Ruder}{28}
\bibcite{Rumelhart}{29}
\bibcite{Sak}{30}
\bibcite{Sharpe}{31}
\bibcite{python_testing}{32}
\bibcite{Smyl}{33}
\bibcite{Treynor}{34}
\bibcite{Winters}{35}
\@writefile{toc}{\contentsline {section}{\numberline {9}\hskip -1em.\nobreakspace  {}Bibliography}{48}{section.9}}
\@writefile{toc}{\contentsline {section}{\numberline {10}\hskip -1em.\nobreakspace  {}Appendix}{49}{section.10}}
\newlabel{libraries}{{22}{49}{Depiction of the list of external packages used to develop the model. These packages are roughly split by use case; mathematical structuring, data visualisation, functionalities required, machine learning tools and miscellaneous}{lstlisting.22}{}}
\@writefile{lol}{\contentsline {lstlisting}{\numberline {22}Depiction of the list of external packages used to develop the model. These packages are roughly split by use case; mathematical structuring, data visualisation, functionalities required, machine learning tools and miscellaneous.}{49}{lstlisting.22}}
\@writefile{toc}{\contentsline {subsection}{\numberline {10.1}\hskip -1em.\nobreakspace  {}Source Code}{49}{subsection.10.1}}
\newlabel{source code}{{10.1}{49}{\hskip -1em.~Source Code}{subsection.10.1}{}}
