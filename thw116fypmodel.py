# -*- coding: utf-8 -*-
"""thw116FYPmodel.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1IHPEI-sPNSS0rKdFThywR4LTpeOt0Z_y
"""

import numpy as np
import pandas as pd
import datetime as dt

import matplotlib.dates as mdates
import matplotlib.pyplot as plt

from sklearn.preprocessing import MinMaxScaler

from keras.models import Sequential
from keras.layers import Dense, LSTM, Dropout, Masking

from keras.utils.vis_utils import model_to_dot
from IPython.display import SVG

from google.colab import files

"""# Initial Data Importation and Structuring

## Accessing and loading data from Google Drive
"""

# mounting Google Drive which contains the relevant data

from google.colab import drive
drive.mount('/content/drive')

# Fundamental input data
ltm_book = pd.read_csv('/content/drive/My Drive/thw116_FYP/data/ltm_book.csv')
ltm_div = pd.read_csv('/content/drive/My Drive/thw116_FYP/data/ltm_div.csv')
ltm_ebit = pd.read_csv('/content/drive/My Drive/thw116_FYP/data/ltm_ebit.csv')
ltm_eps = pd.read_csv('/content/drive/My Drive/thw116_FYP/data/ltm_eps.csv')
ltm_fcf = pd.read_csv('/content/drive/My Drive/thw116_FYP/data/ltm_fcf.csv')
ltm_pbook = pd.read_csv('/content/drive/My Drive/thw116_FYP/data/ltm_pbook.csv')
ltm_sales = pd.read_csv('/content/drive/My Drive/thw116_FYP/data/ltm_sales.csv')

ntm_book = pd.read_csv('/content/drive/My Drive/thw116_FYP/data/ntm_book.csv')
ntm_div = pd.read_csv('/content/drive/My Drive/thw116_FYP/data/ntm_div.csv')
ntm_ebit = pd.read_csv('/content/drive/My Drive/thw116_FYP/data/ntm_ebit.csv')
ntm_eps = pd.read_csv('/content/drive/My Drive/thw116_FYP/data/ntm_eps.csv')
ntm_fcf = pd.read_csv('/content/drive/My Drive/thw116_FYP/data/ntm_fcf.csv')
ntm_pbook = pd.read_csv('/content/drive/My Drive/thw116_FYP/data/ntm_pbook.csv')
ntm_sales = pd.read_csv('/content/drive/My Drive/thw116_FYP/data/ntm_sales.csv')

# Technical input data

price_high = pd.read_csv('/content/drive/My Drive/thw116_FYP/data/price_high.csv')
price_low = pd.read_csv('/content/drive/My Drive/thw116_FYP/data/price_low.csv')
price_open = pd.read_csv('/content/drive/My Drive/thw116_FYP/data/price_open.csv')
volume = pd.read_csv('/content/drive/My Drive/thw116_FYP/data/volume.csv')
enterprise_val = pd.read_csv('/content/drive/My Drive/thw116_FYP/data/enterprise_val.csv')
market_cap = pd.read_csv('/content/drive/My Drive/thw116_FYP/data/market_cap.csv')

print(ltm_book.head())
print("**************************************")
print(ltm_book.info())
print("**************************************")
print(ntm_book.head())
print("**************************************")
print(ntm_book.info())


# Price target output data

price_close = pd.read_csv('/content/drive/My Drive/thw116_FYP/data/price_close.csv')

print(price_close.head())
print("**************************************")
print(price_close.info())



# Test data

# test_data = pd.read_csv("/content/drive/My Drive/thw116_FYP/data/test_price.csv")

# print(test_data.head())
# print("**************************************")
# print(test_data.info())

"""##Restructuring and Standardising the datasets"""

# Filtering incorrect datasets to suitable company subsets
industrials_subset = price_close.columns.values.tolist()

ltm_inputs = [ltm_book, ltm_div, ltm_ebit, ltm_ebitda, ltm_ebitda, ltm_eps, ltm_fcf, ltm_pbook, ltm_sales] 
ntm_inputs = [ntm_book, ntm_div, ntm_ebit, ntm_ebitda, ntm_ebitda, ntm_eps, ntm_fcf, ntm_pbook, ntm_sales]
tech_inputs = [price_high, price_low, price_open, volume, enterprise_val]

for i in range(0,len(ltm_inputs)):
  ltm_inputs[i] = ltm_inputs[i].loc[:, ltm_inputs[i].columns.str.contains('|'.join(industrials_subset))]
  ltm_inputs[i]['Unnamed: 0'] = price_close['Unnamed: 0']
  ltm_inputs[i].rename( columns={'Unnamed: 0':'Date'}, inplace=True )

for i in range(0,len(ntm_inputs)):
  ntm_inputs[i] = ntm_inputs[i].loc[:, ntm_inputs[i].columns.str.contains('|'.join(industrials_subset))]
  ntm_inputs[i]['Unnamed: 0'] = price_close['Unnamed: 0']
  ntm_inputs[i].rename( columns={'Unnamed: 0':'Date'}, inplace=True )

for i in range(0,len(tech_inputs)):
  tech_inputs[i] = tech_inputs[i].loc[:, tech_inputs[i].columns.str.contains('|'.join(industrials_subset))]
  tech_inputs[i]['Unnamed: 0'] = price_close['Unnamed: 0']
  tech_inputs[i].rename( columns={'Unnamed: 0':'Date'}, inplace=True )

  
price_close.rename( columns={'Unnamed: 0':'Date'}, inplace=True )

"""## Data Wrangling

In order to produce a dataframe suitable for the model, we must first take the raw *Price.csv*  file and convert it to returns. This can then be maniuplated further in a variety of ways to achieve the neccessary classification for variants of the model

###Date Index Generator
"""

# Conversion of TimeAndDate column to DatetimeIndex for ease of use

def time_index_generator(dataframe):
  dataframe['Date'] = pd.to_datetime(dataframe['Date'], dayfirst=True)
  dataframe.set_index('Date', inplace=True)


for i in range(0,len(ltm_inputs)):
  time_index_generator(ltm_inputs[i])
for i in range(0,len(ntm_inputs)):
  time_index_generator(ntm_inputs[i])
for i in range(0,len(tech_inputs)):
  time_index_generator(tech_inputs[i])  

time_index_generator(price_close)
  
print(ltm_inputs[0].head())
print(ntm_inputs[0].head())
print(tech_inputs[0].head())

# Check all columns align correctly
print(ltm_inputs[0].columns.difference(price_close.columns))
print(ltm_inputs[1].columns.difference(price_close.columns))
print(ltm_inputs[2].columns.difference(price_close.columns))
print(ltm_inputs[3].columns.difference(price_close.columns))

"""###Ticker Anonymisation"""

# Check that all tickers in the set match

# Helper function to generate ticker list
def ticker_list_generator(num):
  ticker_list = []
  for i in range(0, num):
    ticker_name = 'Ticker ' + str(i+1)
    ticker_list.append(ticker_name)
  return ticker_list

anon_tickers = ticker_list_generator(len(ltm_inputs[0].columns))

for i in range(0,len(ltm_inputs)):
  ltm_inputs[0].columns = anon_tickers
for i in range(0,len(ntm_inputs)):
  ntm_inputs[0].columns = anon_tickers
for i in range(0,len(tech_inputs)):
  tech_inputs[0].columns = anon_tickers

price_close.columns = anon_tickers

print(ltm_inputs[0].head())

# Selecting a Ticker and plotting

# test_data['Ticker 1'].plot(figsize=(16,6))
# plt.title('Test Results - Ticker 1')
# plt.xlabel('Date')
# plt.ylabel('Attribute')

# test_data.plot(figsize=(16,6))

ltm_inputs[0]['Ticker 1'].plot(figsize=(16,6))
plt.title('ltm_book - Ticker 1')
plt.xlabel('Date')
plt.ylabel('Attribute')

"""###Price to Returns Converter"""

# Converting Price format to returns format
def price_to_returns(timeframe, dataframe):
  if timeframe == 'daily':
    return dataframe.pct_change(1) # remember to discount all target variables that are NaN
  
  if timeframe == 'monthly':
    return dataframe.resample('BM')
  
returns = price_to_returns('daily', price_close) 

# returns_data = price_to_returns(price_data)

"""# Data Preprocessing

Several steps are taken in order to structure the data so that it can be proccessed by the model.

1.   Data Analysis and Cleaning: Data is analysed for missing values, its properties, smoothed for various time intervals and resolved of any inconsistencies.  

2. Training-Test Split: Data is divided into training and test splits according to a selected parameter value. 

3.  Data Transformation: Data is normalised, aggregated and generalised, both for training and testing

4. Data Integration: Data is merged together appropriately to form the input shape of the model.

##Data Analysis and Cleaning

Something to consider is the fact that returns are more likely to correlate with changes in fundamentals rather than the absolute values of fundamentals. However, there is likely to be a disceprancy in the rate of change of fundamentals and the change in prices. Fundamentals are often only declared quarterly whereas prices are subject to daily fluctuations. We will first analyse the number of times a fundamental changes relative to the price changes.

###Stastical Summaries of Data Count and Boxplots
"""

# statistical summary of datasets
print(tech_inputs[0].describe())

# Box plot of dataset's datapoints count
tech_inputs[0].count().plot(kind='box', showmeans = True) 
plt.xlabel('Technical inputs')
plt.tight_layout() 
plt.show()

# plt.savefig('tech_box.png')
# files.download('tech_box.png') 


# Individual tickers' datapoint count bar graph

# ltm_inputs[0.count().plot(kind='barh', figsize=(20,20)) 
# plt.xlabel('LTM input')
# plt.tight_layout() 
# plt.show()

# plt.savefig('ltmTickerCount.png')
# files.download('ltmTickerCount.png')

"""###Stastical Summaries of Delta Data Count and Boxplots"""

def data_change(dataframe, column): 
  noChange_count = 0
  NaN_count = 0
  for i in range (0, len(dataframe.index)): 
    if dataframe[column].diff().iloc[i] == 0:
      noChange_count += 1
    if dataframe[column].isna().iloc[i]: 
      NaN_count += 1
  
#   print('Total rows: ', len(dataframe[column]))
#   print('The number of rows where no change occurs: ', noChange_count)
#   print('The number of rows which are NaN: ', NaN_count)
#   print('Useful datapoints:', (len(dataframe[column])-noChange_count-NaN_count))
  return len(dataframe[column])-noChange_count-NaN_count

delta_change = []

for companies in range(0, tech_inputs[0].shape[1]):
  delta_val = data_change(tech_inputs[0], anon_tickers[companies])
  delta_change.append(delta_val)
  

plt.boxplot(delta_change, showmeans = True)
plt.xlabel('Technical inputs')
plt.show()

allreturns = returns.stack(dropna=False).reset_index(drop=True).to_frame('new')
allreturns.hist(range=(0.75,1.25), figsize=(16,8), bins=200)

allreturns = np.log(allreturns) 
allreturns.hist(range=(-0.75,0.75), figsize=(16,8), bins=200)

"""## Training Data Generation"""

def training_set(train_percentage, dataframe): 
  train_size = int(train_percentage*len(dataframe.index)) 
  train_set = dataframe[:train_size]
  return pd.DataFrame(train_set)  

training_split = 0.8

# INPUTS

print(type(ltm_inputs))

ltm_trainInputs = []
ntm_trainInputs = []
tech_trainInputs = []

for i in range(0,len(ltm_inputs)):
  ltm_trainInputs.append(training_set(training_split, ltm_inputs[i]))
  
for i in range(0,len(ntm_inputs)):
  ntm_trainInputs.append(training_set(training_split, ntm_inputs[i]))

for i in range(0,len(tech_inputs)):
  tech_trainInputs.append(training_set(training_split, tech_inputs[i]))

  
returns_trainOutput = training_set(training_split, returns)


print(ntm_trainInputs[0].info())
print(returns_trainOutput.info())


# # TEST INPUT
# test_train_set = training_set(training_split, test_data)
# print("****************INPUT*****************")
# print (test_train_set.head())
# print("**************************************")
# print(test_train_set.info())

"""## Validation Data Generation"""

def validation_set(train_percentage, dataframe): 
  train_size = int(train_percentage*len(dataframe.index)) 
  # val_size = len(dataframe.index)-int(train_percentage*len(dataframe.index)) 
  val_set = dataframe[train_size:]
  return pd.DataFrame(val_set)  


ltm_valInputs = []
ntm_valInputs = []
tech_valInputs = []

for i in range(0,len(ltm_inputs)):
  ltm_valInputs.append(validation_set(training_split, ltm_inputs[i]))
  
for i in range(0,len(ntm_inputs)):
  ntm_valInputs.append(validation_set(training_split, ntm_inputs[i]))

for i in range(0,len(tech_inputs)):
  tech_valInputs.append(validation_set(training_split, tech_inputs[i]))

  
returns_valOutput = validation_set(training_split, returns)

print(ltm_valInputs[0].info())
print(returns_valOutput.info())

"""## Input and Output Transformation

###Scaling
"""

def input_scaling(dataframe):
  
  # This is the MinMax Scaling function 
  sc = MinMaxScaler(feature_range = (0, 1))
  scaled_input_dataframe = sc.fit_transform(dataframe) # This is now an n-dimensional array type
  
  return scaled_input_dataframe

np.warnings.filterwarnings('ignore', r'All-NaN (slice|axis) encountered')

for i in range(0,len(ltm_inputs)):
  ltm_trainInputs[i] = input_scaling(ltm_trainInputs[i])
  ltm_valInputs[i] = input_scaling(ltm_valInputs[i])
  
for i in range(0,len(ntm_inputs)):
  ntm_trainInputs[i] = input_scaling(ntm_trainInputs[i])
  ntm_valInputs[i] = input_scaling(ntm_valInputs[i])

for i in range(0,len(tech_inputs)):
  tech_trainInputs[i] = input_scaling(tech_trainInputs[i])
  tech_valInputs[i] = input_scaling(tech_valInputs[i])
 
# print(type(ltm_trainInputs[0][1000:1001]))
# print(len(ltm_trainInputs[0]))
# print(ltm_trainInputs[0][1000:1001][0][3])
# print(len(ltm_trainInputs[0][:1][0]))

# print(ltm_trainInputs[0][1000:1001])

# sample_input = [ltm_trainInputs[0][1000:1001][0][3],ntm_trainInputs[0][1000:1001][0][3], tech_trainInputs[0][1000:1001][0][3] ]
# print(sample_input)

median = []
median = returns_trainOutput.median(axis=1)

# for column in range(0,returns_trainOutput.shape[0]):
#   for row in range(0, returns_trainOutput.shape[1]): 
#     if returns_trainOutput.iloc[column, row] >= median[row]: 
#       returns_trainOutput.iloc[column, row] = 1
#     else:
#       returns_trainOutput.iloc[column, row] = 0
      
      
# index, row in returns_trainOutput.iterrows():
#     print(index, list(returns_trainOutput.columns[row > median[index]]))

apply(lambda x: 'true' if x <= 2.5 else 'false')

for ticker in range(0,len(returns_trainOutput)):
  if returns_trainOutput[anon_tickers[ticker]] >= median[ticker]
    df['my_channel'].mask(df['my_channel'] > 20000, 0, inplace=True)

def output_classifier(type ,dataframe): 
  
  # No adjustments
  if type == 'raw':
    scaled_dataframe = dataframe
  
  # Binary classifier where return>0 is +1, and return<0 is 0 labels
  if type == 'binary baseline':
    pos_returns = dataframe.values > 0
    neg_returns = dataframe.values <= 0 
    scaled_dataframe = pd.DataFrame(np.select([pos_returns,neg_returns], [1,0], default='NaN'), index=dataframe.index, columns=dataframe.columns)

  return scaled_dataframe

returns_trainOutput = output_classifier('binary baseline', returns_trainOutput)
returns_valOutput = output_classifier('binary baseline', returns_valOutput)

print (returns_trainOutput.head())
print (returns_trainOutput.info())

"""## Data Integration"""

trainInput = []
trainTarget = []

valInput = []
valTarget = []

# Check that the indices are of the same length 
if len(ltm_trainInputs[0]) != len(returns_trainOutput):
  assert False, "Incompatible dataframe index lengths!"

for company in range(0, len(ltm_trainInputs[0][:1][0])):
  
  for time_unit in range(0, len(ltm_trainInputs[0])): 
    input_unit = []
    
    for ltm_attribute in range(0, len(ltm_trainInputs)): 
      input_unit.append(ltm_trainInputs[ltm_attribute][time_unit:time_unit+1][0][company])
                        
    for ntm_attribute in range(0, len(ntm_trainInputs)):
      input_unit.append(ntm_trainInputs[ntm_attribute][time_unit:time_unit+1][0][company])
                        
    for tech_attribute in range(0, len(tech_trainInputs)): 
      input_unit.append(tech_trainInputs[tech_attribute][time_unit:time_unit+1][0][company])
                        
    trainInput.append(input_unit)
    
for company in range(0, len(returns_trainOutput.columns)):
  for time_unit in range(0, len(returns_trainOutput.index)): 
    trainTarget.append(returns_trainOutput[anon_tickers[company]][time_unit])
  
  
for company in range(0, len(ltm_valInputs[0][:1][0])):
  
  for time_unit in range(0, len(ltm_valInputs[0])): 
    input_unit = []
    
    for ltm_attribute in range(0, len(ltm_valInputs)): 
      input_unit.append(ltm_valInputs[ltm_attribute][time_unit:time_unit+1][0][company])
                        
    for ntm_attribute in range(0, len(ntm_trainInputs)):
      input_unit.append(ntm_valInputs[ntm_attribute][time_unit:time_unit+1][0][company])
                        
    for tech_attribute in range(0, len(tech_trainInputs)): 
      input_unit.append(tech_valInputs[tech_attribute][time_unit:time_unit+1][0][company])
                        
    valInput.append(input_unit)
    
for company in range(0, len(returns_valOutput.columns)):
  for time_unit in range(0, len(returns_valOutput.index)): 
    valTarget.append(returns_valOutput[anon_tickers[company]][time_unit])

# for i in range(0, len(scaled_returns_train_set.index)): # this is the ratio of input data to output data. 
#     X_train.append(scaled_test_train_set[i-6:i, 0]) # second parameter is the axis - in this case, only 1 dimension
#     Y_train.append(scaled_returns_train_set[i, 0])

# print(type(input_trainDataset))
# print("**************************************")
# print(input_trainDataset[0:5])


# # Conversion to numpy array for improved memory, performance and functionality
# input_trainDataset, output_trainDataset = np.array(input_trainDataset), np.array(output_trainDataset)
# print(type(input_trainDataset))
# print("**************************************")

# print(input_trainDataset[0:5])

# print(input_trainDataset.shape[0])
# print(input_trainDataset.shape[1])
# print("**************************************")


print(trainInput[0:1])
print(len(trainInput))
print(trainTarget[0:1])
print(len(trainTarget))

print(valInput[0:1])
print(len(valInput))
print(valTarget[0:1])
print(len(valTarget))

"""### Remove redundant data"""

def remove_useless_inputs(inputList, outputList):
  resultInput = []
  resultOutput = []
  for i in range (0, len(inputList)):
    nancount = 0
    for val in range(0, len(inputList[i])):
      if str(inputList[i:i+1][0][val]) == 'nan': 
        nancount += 1
#     if nancount < len(inputList[i:i+1][0]): 
    if nancount < 1:
      resultInput.append(inputList[i])
      resultOutput.append(outputList[i])
      
  return resultInput, resultOutput

newtrainInput, newtrainTarget = remove_useless_inputs(trainInput, trainTarget)

print(newtrainInput[0:1])
print(len(newtrainInput))
print(newtrainTarget[0:1])
print(len(newtrainTarget))

def remove_useless_outputs(inputList, outputList):
  resultInput = []
  resultOutput = []
  for i in range (0, len(outputList)):
    if str(outputList[i:i+1][0]).lower() != 'nan': 
      resultInput.append(inputList[i])
      resultOutput.append(outputList[i])
   
  if isinstance(resultOutput[0:1][0], str):
    for x in range (0, len(resultOutput)):
      resultOutput[x:x+1] = list(map(int, resultOutput[x:x+1][0]))
  print(type(resultOutput[0:1][0]))

  print(type(resultOutput[0:1][0]))
  return resultInput, resultOutput

trainInput, trainTarget = remove_useless_outputs(newtrainInput, newtrainTarget)

print(trainInput[0:1])
print(len(trainInput))
print(trainTarget[0:1])
print(len(trainTarget))


print(type(trainTarget[0:1]))

print(trainTarget[0:1])
print(trainTarget[0])
print(trainTarget[1])

"""### Reshaping data"""

# Conversion to numpy array for improved memory, performance and functionality
trainInput, trainTarget = np.array(trainInput), np.array(trainTarget)
print(trainInput[0:1])

print(trainInput.shape[0])
print(trainInput.shape[1])

trainInput = np.reshape(trainInput, (trainInput.shape[0], trainInput.shape[1], 1))

print(trainInput[0:1])

print(trainInput.shape[0])
print(trainInput.shape[1])

"""# Network Implementation

## Core Architecture
"""

data_dim = trainInput.shape[1]
timesteps = trainInput.shape[0]

# Sample Code
# model parameters:

def create_model(train_X, train_Y, data_dim):
  lstm_units = 128
  
#   print('Build baseline binary model...')
#   model = Sequential()
#   model.add(Masking(mask_value=0., input_shape=(data_dim, 1)))
#   model.add(LSTM(lstm_units))
#   model.add(Dense(1))
#   model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])
  
  
  print('Build stacked binary model')
  model = Sequential()
  model.add(Masking(mask_value=0., input_shape=(data_dim, 1)))
  model.add(LSTM(lstm_units, return_sequences=True))
  model.add(Dropout(rate=0.2))
  model.add(LSTM(lstm_units, return_sequences=True))
  model.add(Dropout(rate=0.2))

  model.add(LSTM(lstm_units))
  model.add(Dense(1))
  model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])

  
  #   model.compile(loss='mean_squared_error', optimizer='adam', metrics=['accuracy'])

  return(model)

  
baseline_model = create_model(trainInput, trainTarget, data_dim)
SVG(model_to_dot(baseline_model, show_shapes=True).create(prog='dot', format='svg'))

baseline_model.summary()
baseline_model.fit(trainInput, trainTarget, epochs = 100, batch_size = 256, verbose = 1)

data_dim = trainInput.shape[1]
timesteps = trainInput.shape[0]


class fypNet: 
  @staticmethod
  def build_input_predictor(inputs, data_dim, lstm_units):
    inPred = Masking(mask_value=0., input_shape = (data_dim, 1))(inputs)
    inPred = LSTM(lstm_units, return_sequences=True)

    result = Activation('softmax', name= 'inPred_result')(inPred)
    
    return result
    
  @staticmethod
  def build_output_predictor(inputs, data_dim, lstm_units): 
    outPred = LSTM(lstm_units, return_sequences=True)(inputs)
    outPred = LSTM(lstm_units)(outPred)
    outPred = Dense(1)(outPred)
    result = Activation('softmax', name= 'outPred_result')(outPred)

    return result
  
  @staticmethod
  def build():
    input_shape = (data_dim, inputs, lstm_units)
    
    inputs = Input(shape = input_shape)
 
    inputBranch = fypNet.build_input_predictor(params)
    outputBranch = fypNet.build_output_predictor(params)
    
    model = Model(inputs = inputs, outputs = [inputBranch, outputBranch])
    
    return model

data_dim = trainInput.shape[1]
timesteps = trainInput.shape[0]
lstm_units = 128

num_epochs = 30
initial_lr = 1e-3
batch_size = 32

# initialize our FashionNet multi-output network
model = fypNet.build(data_dim, trainInput, lstm_units)
 
losses = {'inPred_result': 'mean_absolute_error','outPred_result': 'binary_crossentropy'}
 
# initialize the optimizer and compile the model
print('Compiling model...')
optimiser = Adam(lr=initial_lr, decay=initial_lr/epochs)
model.compile(optimizer=opt, loss=losses, metrics=['accuracy'])

SVG(model_to_dot(baseline_model, show_shapes=True).create(prog='dot', format='svg'))
baseline_model.summary()


model.fit(trainInput,{"inPred_result": , "outPred_result": trainTarget}, validation_data=(testInput, {"inPred_result": , "outPred_result": trainTarget}), epochs=num_epochs, verbose=1)

class FashionNet:
  @staticmethod
  def build_category_branch(inputs, numCategories, finalAct="softmax",chanDim=-1):    
    x = Lambda(lambda c: tf.image.rgb_to_grayscale(c))(inputs)
		# utilize a lambda layer to convert the 3 channel input to a
		# grayscale representation
 
		# CONV => RELU => POOL
		x = Conv2D(32, (3, 3), padding="same")(x)
		x = Activation("relu")(x)
		x = BatchNormalization(axis=chanDim)(x)
		x = MaxPooling2D(pool_size=(3, 3))(x)
		x = Dropout(0.25)(x)
    
    	# (CONV => RELU) * 2 => POOL
		x = Conv2D(64, (3, 3), padding="same")(x)
		x = Activation("relu")(x)
		x = BatchNormalization(axis=chanDim)(x)
		x = Conv2D(64, (3, 3), padding="same")(x)
		x = Activation("relu")(x)
		x = BatchNormalization(axis=chanDim)(x)
		x = MaxPooling2D(pool_size=(2, 2))(x)
		x = Dropout(0.25)(x)
 
		# (CONV => RELU) * 2 => POOL
		x = Conv2D(128, (3, 3), padding="same")(x)
		x = Activation("relu")(x)
		x = BatchNormalization(axis=chanDim)(x)
		x = Conv2D(128, (3, 3), padding="same")(x)
		x = Activation("relu")(x)
		x = BatchNormalization(axis=chanDim)(x)
		x = MaxPooling2D(pool_size=(2, 2))(x)
		x = Dropout(0.25)(x)
    
    # define a branch of output layers for the number of different
		# clothing categories (i.e., shirts, jeans, dresses, etc.)
		x = Flatten()(x)
		x = Dense(256)(x)
		x = Activation("relu")(x)
		x = BatchNormalization()(x)
		x = Dropout(0.5)(x)
		x = Dense(numCategories)(x)
		x = Activation(finalAct, name="category_output")(x)
 
		# return the category prediction sub-network
		return x
  
  @staticmethod
	def build_output_predictor(inputs, numColors, finalAct="softmax", chanDim=-1):
		# CONV => RELU => POOL
		x = Conv2D(16, (3, 3), padding="same")(inputs)
		x = Activation("relu")(x)
		x = BatchNormalization(axis=chanDim)(x)
		x = MaxPooling2D(pool_size=(3, 3))(x)
		x = Dropout(0.25)(x)
 
		# CONV => RELU => POOL
		x = Conv2D(32, (3, 3), padding="same")(x)
		x = Activation("relu")(x)
		x = BatchNormalization(axis=chanDim)(x)
		x = MaxPooling2D(pool_size=(2, 2))(x)
		x = Dropout(0.25)(x)
 
		# CONV => RELU => POOL
		x = Conv2D(32, (3, 3), padding="same")(x)
		x = Activation("relu")(x)
		x = BatchNormalization(axis=chanDim)(x)
		x = MaxPooling2D(pool_size=(2, 2))(x)
		x = Dropout(0.25)(x)
    
    # define a branch of output layers for the number of different
		# colors (i.e., red, black, blue, etc.)
		x = Flatten()(x)
		x = Dense(128)(x)
		x = Activation("relu")(x)
		x = BatchNormalization()(x)
		x = Dropout(0.5)(x)
		x = Dense(numColors)(x)
		x = Activation(finalAct, name="color_output")(x)
 
		# return the color prediction sub-network
		return x
  
  @staticmethod
    def build(width, height, numCategories, numColors, finalAct="softmax"):
      # initialize the input shape and channel dimension (this code
      # assumes you are using TensorFlow which utilizes channels
      # last ordering)
      inputShape = (height, width, 3)
      chanDim = -1

      # construct both the "category" and "color" sub-networks
      inputs = Input(shape=inputShape)
      categoryBranch = FashionNet.build_category_branch(inputs,
        numCategories, finalAct=finalAct, chanDim=chanDim)
      colorBranch = FashionNet.build_color_branch(inputs,
        numColors, finalAct=finalAct, chanDim=chanDim)

      # create the model using our input (the batch of images) and
      # two separate outputs -- one for the clothing category
      # branch and another for the color branch, respectively
      model = Model(
        inputs=inputs,
        outputs=[categoryBranch, colorBranch],
        name="fashionnet")

      # return the constructed network architecture
      return model

# initialize our FashionNet multi-output network
model = FashionNet.build(96, 96,
	numCategories=len(categoryLB.classes_),
	numColors=len(colorLB.classes_),
	finalAct="softmax")
 
# define two dictionaries: one that specifies the loss method for
# each output of the network along with a second dictionary that
# specifies the weight per loss
losses = {
	"category_output": "categorical_crossentropy",
	"color_output": "categorical_crossentropy",
}
lossWeights = {"category_output": 1.0, "color_output": 1.0}
 
# initialize the optimizer and compile the model
print("[INFO] compiling model...")
opt = Adam(lr=INIT_LR, decay=INIT_LR / EPOCHS)
model.compile(optimizer=opt, loss=losses, loss_weights=lossWeights, metrics=["accuracy"])

"""# Benchmark"""